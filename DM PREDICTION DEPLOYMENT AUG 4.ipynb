{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# Deep Learning to solve a Client Success problem - Duty Manager call prediction deployment notebook\n\n### Goal: predict tickets that will generate a Duty Manager (DM) call\n\n- ~1.5% of Db2 tickets generate DM calls; each call generates additional work/impedence for the customer, the analyst, and the Duty Manager\n- if we could predict which tickets will generate DM calls, and take proactive actions on those tickets (e.g. proactively call customer), all stakeholders benefit\n- train a simple deep learning model on Db2 ticket data Feb 1 - May 20, 2018\n- this notebook applies that model to tickets post May 20\n\n### Features\n- categorical: CASE_OWNER_ALIAS, SUPPORT_MISSION, CREATED_BY, ACCOUNT_NAME, LEGACY_PROBLEM_NUMBER, SEVERITY_LEVEL_NUMBER_FORMULA, OPERATING_SYSTEM, PRODUCT_NAME, ACCOUNT_PRIORITY, PRODUCT_VERSION, BLUE_DIAMOND_ACCOUNT\n- continuous: OWNERSHIP_CHANGES\n- text: SUBJECT\n\n### Label / Target:\n\n- will the ticket generate a DM call? 0 if no; 1 if yes\n\n### Current HWMs: 87% accuracy on validation; 81.5% on \"wild\" data\n\n### Supported Versions for deployment: \n- Keras version: 2.1.3\n- TensorFlow version: 1.5\n\n### Functionalities covered in this notebook:\n- load a trained Keras model\n- Save the trained Keras model to WML Repository\n- Deploy and score using the saved model\n\n### Pre-requisites to deploy a Keras model\n- Keras's save() API should be used for saving the trained model.\n- The saved model's .h5/.hdf5 file must be archived and compressed in tar.gz(/.tgz) format. The .h5/.hdf5 file must be at the first level in the compressed archive. \n- The compressed archive file must be saved in the WML Repository.\n\n\n### Next steps to try:\n\n- update model to incorporate DATE_OPENED feature\n- deploy and maintain in production\n\nDL Glossary: https://deeplearning4j.org/glossary\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Proposed end-to-end deployment of model\n<table style=\"border: none\" align=\"left\">\n   </tr>\n   <tr style=\"border: none\">\n       <th style=\"border: none\"><img src=\"https://raw.githubusercontent.com/ryanmark1867/machine-learning-may-2018/ml-project-edits/endtoend.jpg\" width=\"600\" alt=\"Icon\"> </th>\n   </tr>\n</table>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Data transformations\n<table style=\"border: none\" align=\"left\">\n   </tr>\n   <tr style=\"border: none\">\n       <th style=\"border: none\"><img src=\"https://raw.githubusercontent.com/ryanmark1867/machine-learning-may-2018/ml-project-edits/datatransformationsjune11.jpg\" width=\"600\" alt=\"Icon\"> </th>\n   </tr>\n</table>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Visualization of model\n<table style=\"border: none\" align=\"left\">\n   </tr>\n   <tr style=\"border: none\">\n       <th style=\"border: none\"><img src=\"https://raw.githubusercontent.com/ryanmark1867/machine-learning-may-2018/master/dmmodelplotjune9annot.png\" width=\"600\" alt=\"Icon\"> </th>\n   </tr>\n</table>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Links to key parts of the notebook <a name='linkanchor' />\n<a href=#ingestdash>Ingest data and load model</a>\n\n<a href=#definecategories>Define feature categories</a>\n\n<a href=#bookmark>Deal with missing values</a>\n\n<a href=#modelfit>Define Keras variables</a>\n\n<a href=#predrend>Predictions and renderings</a>\n\n<a href=#confusionmatrix>Confusion matrix</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
                }
            ], 
            "source": "import zipfile\nimport pandas as pd\nimport numpy as np\nimport time\nimport datetime\nfrom dateutil import relativedelta\nfrom io import StringIO\nimport pandas as pd\nfrom keras.models import model_from_json\nfrom keras.models import load_model\nfrom keras.models import model_from_yaml\nimport pickle\nimport tensorflow as tf\nimport keras\n# from sklearn.preprocessing import CategoricalEncoder\n# DSX code to import uploaded documents\nfrom io import StringIO\nimport requests\nimport json\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom keras.utils.vis_utils import plot_model\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport math\nfrom subprocess import check_output\nimport seaborn as sns\nimport types\nimport tempfile\nimport keras.models\nfrom keras.preprocessing.text import Tokenizer"
        }, 
        {
            "source": "# Validate Keras level", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting keras==2.1.3\n  Downloading https://files.pythonhosted.org/packages/08/ae/7f94a03cb3f74cdc8a0f5f86d1df5c1dd686acb9a9c2a421c64f8497358e/Keras-2.1.3-py2.py3-none-any.whl (319kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 327kB 4.0MB/s eta 0:00:01\n\u001b[?25hRequirement not upgraded as not directly required: scipy>=0.14 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras==2.1.3)\nRequirement not upgraded as not directly required: numpy>=1.9.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras==2.1.3)\nRequirement not upgraded as not directly required: six>=1.9.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras==2.1.3)\nRequirement not upgraded as not directly required: pyyaml in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from keras==2.1.3)\nInstalling collected packages: keras\n  Found existing installation: Keras 2.1.4\n    Uninstalling Keras-2.1.4:\n      Successfully uninstalled Keras-2.1.4\nSuccessfully installed keras-2.1.3\n"
                }
            ], 
            "source": "!pip install keras==2.1.3"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Name: Keras\nVersion: 2.1.3\nSummary: Deep Learning for humans\nHome-page: https://github.com/keras-team/keras\nAuthor: Francois Chollet\nAuthor-email: francois.chollet@gmail.com\nLicense: MIT\nLocation: /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequires: scipy, pyyaml, six, numpy\n---\nName: tensorflow\nVersion: 1.3.0\nSummary: TensorFlow helps the tensors flow\nHome-page: http://tensorflow.org/\nAuthor: Google Inc.\nAuthor-email: opensource@google.com\nLicense: Apache 2.0\nLocation: /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages\nRequires: tensorflow-tensorboard, numpy, wheel, six, protobuf\n"
                }
            ], 
            "source": "!pip show keras tensorflow"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def bootstrap():\n    \n    global dsxmode, csvmode, dbmode, pSpace, testproportion, trainproportion, verboseout, includetext, presaved, savemodel\n    global remakeTokenizer, hctextmax, maxwords, BATCH_SIZE, savedle, targetthresh, emptythresh, class_weight\n    global zero_weight, one_weight, hyperparameters, learning_rate, dropout_rate, l2_lambda\n    dsxmode = False # overall is this being run in DSX?\n    csvmode = True # ingest from CSV\n    dbmode = False # ingest from database\n    pSpace = False # pSpace mode\n\n    testproportion = 0.01 # proportion of data reserved for test set\n    trainproportion = 0.99 # proportion of non-test data dedicated to training (vs. validation)\n    verboseout = True\n    includetext = True # switch to determine whether text fields are included in model\n    presaved = True # switch to determine whether to train model or load saved model\n    savemodel = True # switch to determine whether to save model\n    remakeTokenizer = False # switch to determine whether to rebuild tokenizer based on input in this notebook or use imported tokenizer\n    hctextmax = 7000\n    maxwords = 6000\n    BATCH_SIZE = 200\n    savedle = True # switch for whether a pickled label encoder is used\n\n    targetthresh = 6.0\n    emptythresh = 1000\n    # to address imbalance in training data between zero (above targetthresh) and detractor (below targetthresh) specify weight in compile and fit\n    # class_weight = {0 : zero_weight, 1: one_weight}\n    # consider calculating these values from actual skew rather than hard-coding them here\n    zero_weight = 1.0\n    one_weight = 72.8\n\n    # hyperparameters\n    learning_rate = 0.001\n    dropout_rate = 0.00003 #0.003\n    l2_lambda = 0.00003 #7.5\n"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "# Ingest data and load model<a name='ingestdash' />\n<a href=#linkanchor>Back to link list</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# ingest data from duty manager report and all cases for the scoring period\ndef ingest_data():\n    global dm_cases, merged_data\n    dm_cases = pd.read_csv(project.get_file('dmcasesjun22018.csv'),encoding = \"ISO-8859-1\")\n    merged_data = pd.read_csv(project.get_file('alldmeracasesjune22018posttrain.csv'),encoding = \"ISO-8859-1\")\n    merged_data.columns = map(str.upper,merged_data.columns)\n    dm_cases.columns = map(str.upper,dm_cases.columns)"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#loading model, weights, tokenizer and label encoder\n\ndef load_items():\n    loaded_model = pickle.load(project.get_file('pickledmodelaug4.pickle'))\n    #load_path = \"dmmodelaug2.h5\"\n    #loaded_model = load_model(load_path)\n    if remakeTokenizer == False:\n        tok_raw = pickle.load(project.get_file('tokenizeraug4.pickle'))\n        # workaround for issue serializing tokenizer\n    tok_raw.oov_token = None\n    lelist = pickle.load(project.get_file('leaug4.pickle'))\n    nearempty = pickle.load(project.get_file('neaug4.pickle'))\n    print(\"len(lelist)\",len(lelist))\n    return(loaded_model, tok_raw, lelist, nearempty)"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# create merged dataframe\ndef create_merged_df(dm_cases, merged_data):\n    # for all rows in the dm_cases dataframe set the target to 1\n    dm_cases[\"target\"] = 1\n    # defined columns to keep from dm_cases\n    dmcollist = list(dm_cases)\n    dmexlist = ['CASE_NUMBER','target']\n    dmcollist = list(set(dmcollist) - set(dmexlist))\n    # drop extraneous columns from dm_cases\n    dm_cases.drop(dmcollist, inplace=True, axis=1)\n    print(\"dm_cases cols post pruning\",list(dm_cases))\n    print(\"dm_cases.target.value_counts()\",dm_cases.target.value_counts())\n    print(\"dm_cases.shape\",dm_cases.shape)\n    # join the dm_cases dataframe with the overall cases dataframe\n    merged_data = merged_data.join(dm_cases.set_index('CASE_NUMBER'), on = 'CASE_NUMBER')\n    print(\"merged_data.shape\",merged_data.shape)\n    print(\"merged_data.target.value_counts()\",merged_data.target.value_counts())\n    # fill target column with 0 if not already 1\n    merged_data['target']=merged_data['target'].fillna(0.0)\n    print(\"merged_data.target.value_counts()\",merged_data.target.value_counts())\n    print(\"merged_data.shape\",merged_data.shape)\n    # clean up LEGACY_PROBLEM_NUMBER\n    merged_data['LEGACY_PROBLEM_NUMBER'] = np.where(merged_data['LEGACY_PROBLEM_NUMBER'] == 'Not Applicable','Not Applicable','PMR')\n    if presaved == True:\n    # if the model is saved all data gets processed\n        train = merged_data\n        print(\"presaved model - not training/test split\")\n    else:\n        train, test = train_test_split(merged_data, test_size = testproportion)\n        print(testproportion)\n        print(\"Through train test split. Test proportion:\")\n    return(merged_data, train)"
        }, 
        {
            "source": "# Define feature categories <a name='definecategories' />\n<a href=#linkanchor>Back to link list</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# define the column lists for categorical, continuous, and text features, as well as columns to exclude from running through the model\n\ndef define_feature_categories(merged_data):\n    allcols = list(merged_data)\n    print(\"all cols\",allcols)\n    textcols = ['SUBJECT'] # columns to deal with as text - replace entries with multiple IDs and use embeddings, RNN\n    continuouscols = ['OWNERSHIP_CHANGES'] # columns to deal with as continuous values - no embeddings\n    excludefromcolist = ['CASE_NUMBER','target','OPENED_DATE'] # columns to exclude completely from the model\n    # list(set(temp1) - set(temp2))\n    nontextcols = list(set(allcols) - set(textcols))\n    collist = list(set(nontextcols) - set(excludefromcolist) - set(nearempty) - set(continuouscols))\n    # ensure continuous categories have numeric type\n    for col in continuouscols:\n        merged_data[col] = merged_data[col].astype(float)\n\n    # print column list contents:\n    print(\"allcols\",allcols)\n    print(\"nearempty\",nearempty)\n    print(\"excludefromcolist\",excludefromcolist)\n    print(\"textcols\",textcols)\n    print(\"continuouscols\",continuouscols)\n    # Aug 17 hardcode collist for testing\n    #collist = ['OPERATING_SYSTEM', 'ACCOUNT_PRIORITY', 'BLUE_DIAMOND_ACCOUNT', 'SEVERITY_LEVEL_NUMBER_FORMULA', 'PRODUCT_VERSION', 'PRODUCT_NAME', 'SUPPORT_MISSION', 'CREATED_BY', 'ACCOUNT_NAME', 'LEGACY_PROBLEM_NUMBER', 'CASE_OWNER_ALIAS']\n    # Aug 28 hardcode collist to match order of input layers in model summary below\n    # collist = ['ACCOUNT_NAME','CASE_OWNER_ALIAS','SEVERITY_LEVEL_NUMBER_FORMULA','PRODUCT_NAME','BLUE_DIAMOND_ACCOUNT','LEGACY_PROBLEM_NUMBER','PRODUCT_VERSION','CREATED_BY','SUPPORT_MISSION','ACCOUNT_PRIORITY','OPERATING_SYSTEM']\n    # Aug 31 experiment - flip 2nd and 8th cols\n    #collist = ['ACCOUNT_NAME','CREATED_BY','SEVERITY_LEVEL_NUMBER_FORMULA','PRODUCT_NAME','BLUE_DIAMOND_ACCOUNT','LEGACY_PROBLEM_NUMBER','PRODUCT_VERSION','CASE_OWNER_ALIAS','SUPPORT_MISSION','ACCOUNT_PRIORITY','OPERATING_SYSTEM']\n    \n    # Aug 31 - copy exact order of collist from model build\n    # collist = ['OPERATING_SYSTEM','ACCOUNT_PRIORITY','BLUE_DIAMOND_ACCOUNT','SEVERITY_LEVEL_NUMBER_FORMULA','PRODUCT_VERSION','PRODUCT_NAME','SUPPORT_MISSION','CREATED_BY','ACCOUNT_NAME','LEGACY_PROBLEM_NUMBER','CASE_OWNER_ALIAS']\n    # SEPT 2 - TRANSPOSE TWO PROBLEM COLS - PRODUCT_VERSION into LEGACY_PROBLEM_NUMBER\n    # collist = ['OPERATING_SYSTEM','ACCOUNT_PRIORITY','BLUE_DIAMOND_ACCOUNT','SEVERITY_LEVEL_NUMBER_FORMULA','LEGACY_PROBLEM_NUMBER','PRODUCT_NAME','SUPPORT_MISSION','CREATED_BY','ACCOUNT_NAME','PRODUCT_VERSION','CASE_OWNER_ALIAS']\n    #collist = ['OPERATING_SYSTEM','ACCOUNT_PRIORITY','BLUE_DIAMOND_ACCOUNT','SEVERITY_LEVEL_NUMBER_FORMULA','PRODUCT_VERSION','PRODUCT_NAME','SUPPORT_MISSION','CREATED_BY','ACCOUNT_NAME','LEGACY_PROBLEM_NUMBER','CASE_OWNER_ALIAS']\n    # exact copy of order of columns from summary of model def notebook\n    collist = ['ACCOUNT_NAME','CASE_OWNER_ALIAS','SEVERITY_LEVEL_NUMBER_FORMULA','PRODUCT_NAME','BLUE_DIAMOND_ACCOUNT','LEGACY_PROBLEM_NUMBER','PRODUCT_VERSION','CREATED_BY','SUPPORT_MISSION','ACCOUNT_PRIORITY','OPERATING_SYSTEM']\n    \n    # Sept 3 try reversing order - nope\n    # collist = ['OPERATING_SYSTEM','ACCOUNT_PRIORITY','SUPPORT_MISSION','CREATED_BY','PRODUCT_VERSION','LEGACY_PROBLEM_NUMBER','BLUE_DIAMOND_ACCOUNT','PRODUCT_NAME','SEVERITY_LEVEL_NUMBER_FORMULA','CASE_OWNER_ALIAS','ACCOUNT_NAME']\n    # experiment putting BD first\n    # collist = ['BLUE_DIAMOND_ACCOUNT','ACCOUNT_NAME','CASE_OWNER_ALIAS','SEVERITY_LEVEL_NUMBER_FORMULA','PRODUCT_NAME','LEGACY_PROBLEM_NUMBER','PRODUCT_VERSION','CREATED_BY','SUPPORT_MISSION','ACCOUNT_PRIORITY','OPERATING_SYSTEM']\n    # collist = ['BLUE_DIAMOND_ACCOUNT','ACCOUNT_NAME','CASE_OWNER_ALIAS','SEVERITY_LEVEL_NUMBER_FORMULA','PRODUCT_NAME','LEGACY_PROBLEM_NUMBER','PRODUCT_VERSION','CREATED_BY','SUPPORT_MISSION','ACCOUNT_PRIORITY','OPERATING_SYSTEM']\n    # aug 31 experiment - blows up\n    # collist = ['BLUE_DIAMOND_ACCOUNT','BLUE_DIAMOND_ACCOUNT','BLUE_DIAMOND_ACCOUNT','BLUE_DIAMOND_ACCOUNT','BLUE_DIAMOND_ACCOUNT','BLUE_DIAMOND_ACCOUNT','BLUE_DIAMOND_ACCOUNT','BLUE_DIAMOND_ACCOUNT','BLUE_DIAMOND_ACCOUNT','BLUE_DIAMOND_ACCOUNT','BLUE_DIAMOND_ACCOUNT']\n    print(\"collist\",collist)\n    return(allcols,textcols,continuouscols,collist)"
        }, 
        {
            "source": "# Deal with missing values and encode categorical values and text <a name='bookmark' />\n<a href=#linkanchor>Back to link list</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# fill missing values according to column type\n\ndef fill_missing(dataset):\n    print(\"before mv\")\n    for col in collist:\n        dataset[col].fillna(value=\"missing\", inplace=True)\n    for col in continuouscols:\n        dataset[col].fillna(value=0.0,inplace=True)\n    for col in textcols:\n        dataset[col].fillna(value=\"missing\", inplace=True)\n    return (dataset)\n"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# process categorical data\n# replace values with category IDs in the following columns\n# with the label encoder being read from LE used in training, do not fit again\n\ndef encode_labels(train,lelist):\n    le = LabelEncoder()\n    train['SUBJECT'].head(15)\n    if savedle == True:\n        # use lelist which was read from pickled version saved at model building time\n        for col in collist:\n            print(\"processing \",col)\n            # le.fit(np.hstack([train[col], test[col]]))\n            train[col] = lelist[col].transform(train[col])\n    else:\n        # create le based on test input                                      \n        for col in collist:\n            if verboseout:\n                print(\"processing \",col)\n            le.fit(np.hstack([train[col], test[col]]))\n            train[col] = le.transform(train[col])\n    train.iloc[:,0:13].head()\n    return(train)"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# tokenize list of text columns (made up of multiple strings)\n\nfrom keras.preprocessing.text import Tokenizer\n\ndef encode_text(train, tok_raw):\n    if remakeTokenizer == True:\n        for col in textcols:\n            if verboseout:\n                print(\"processing text col\",col)\n            # Tokenizer lower cases and removes punctuation by default\n            tok_raw = Tokenizer(num_words=maxwords,lower=True)\n            tok_raw.fit_on_texts(train[col])\n            train[col] = tok_raw.texts_to_sequences(train[col])\n    else:\n        for col in textcols:\n            if verboseout:\n                print(\"processing text col\",col)\n            # Tokenizer lower cases and removes punctuation by default\n            print(\"using loaded tokenizer\")\n            train[col] = tok_raw.texts_to_sequences(train[col])\n    train.iloc[:,0:13].head()\n    return(train)"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# calculate max values for embeddings\n\n\ndef get_max_embeddings(train):\n    max_dict = {}\n    textmax = 50\n    i = 0\n    for col in collist:\n        \n        max_dict[col] = np.max([train[col].max(), train[col].max()])+ 1\n        # max_dict[col] = np.max([train[col].max(), train[col].max()])+2\n        print(\"col in max is\", col)\n        print(\"max is\", max_dict[col])\n        print(\"i is \",i)\n        i = i+1\n\n    # np.max([np.max(train['Resolution_Description'].max()), np.max(train['Subject'].max()),np.max(train['Other_Reason_for_Cancellation'].max()),np.max(train['Reason_for_Reopening'].max())])) \n    for cols in textcols:\n        # updated Aug 5 to clean up size mismatch\n        max_dict[cols] = maxwords - 1 \n        print(\"col is\", cols)\n        print(\"max is\", max_dict[cols])\n        '''\n        maxtrain = np.max(train[(train[cols].map(len) != 0)][cols].map(max))\n        #maxtest = np.max(test[(test[cols].map(len) != 0)][cols].map(max))\n        if verboseout:\n            print(\"maxtrain.max()\",maxtrain)\n            #print(\"maxtest .max()\",maxtest)\n        max_dict[cols] = maxtrain\n        if max_dict[cols] > textmax:\n            textmax = max_dict[cols]\n        '''\n\n    if textmax < hctextmax:\n        textmax = hctextmax\n    print(\"textmax\",textmax)\n    if verboseout:\n        print(\"max_dict\",max_dict)\n    # maxwords\n    return(max_dict, textmax)\n    # return(maxwords, textmax)"
        }, 
        {
            "source": "# Define Keras variables <a name='modelfit' />\n<a href=#linkanchor>Back to link list</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# define keras variables\n\n# define keras variables\nfrom keras.preprocessing.sequence import pad_sequences\n\n# X for the features used\n\ndef get_keras_vars(dataset):\n    X = {}\n    dictlist = []\n    i = 0\n    for col in collist:\n        if verboseout:\n            print(\"cat col is\",col)\n            \n        X[col] = np.array(dataset[col])\n        dictlist.append(np.array(dataset[col]))\n       \n    for col in textcols:\n        if verboseout:\n            print(\"text col is\",col)\n        X[col] = pad_sequences(dataset[col], maxlen=max_dict[col])\n        dictlist.append(pad_sequences(dataset[col], maxlen=max_dict[col]))\n        \n    for col in continuouscols:\n        if verboseout:\n            print(\"cont col is\",col)\n        X[col] = np.array(dataset[col])\n        dictlist.append(np.array(dataset[col]))\n        \n    return X, dictlist\n\ndef get_keras_list_only(X_in):\n    dictlist = []\n    for key, value in X_in.items():\n        print(\"X def loop key\",key)\n        print(\"value shape\",value.shape)\n        temp = [key,value]\n        dictlist.append(value)\n    return dictlist\n\ndef get_keras_np(X_in):\n    return np.array(list(X_in.items()),dtype=object)\n# np.array(list(result.items()), dtype=dtype)\n\n# the deployment API for Watson Studio can only take a list/array, not a dictionary, so define list-only version for input\n\n\n"
        }, 
        {
            "source": "# Predictions and renderings <a name='predrend' />\n<a href=#linkanchor>Back to link list</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "len(lelist) 11\ndm_cases cols post pruning ['CASE_NUMBER', 'target']\ndm_cases.target.value_counts() 1    121\nName: target, dtype: int64\ndm_cases.shape (121, 2)\nmerged_data.shape (1041, 16)\nmerged_data.target.value_counts() 1.0    11\nName: target, dtype: int64\nmerged_data.target.value_counts() 0.0    1030\n1.0      11\nName: target, dtype: int64\nmerged_data.shape (1041, 16)\npresaved model - not training/test split\nall cols ['CASE_NUMBER', 'CASE_OWNER_ALIAS', 'OPENED_DATE', 'SUPPORT_MISSION', 'PRODUCT_NAME', 'ACCOUNT_PRIORITY', 'OWNERSHIP_CHANGES', 'SEVERITY_LEVEL_NUMBER_FORMULA', 'ACCOUNT_NAME', 'BLUE_DIAMOND_ACCOUNT', 'PRODUCT_VERSION', 'OPERATING_SYSTEM', 'SUBJECT', 'CREATED_BY', 'LEGACY_PROBLEM_NUMBER', 'target']\nallcols ['CASE_NUMBER', 'CASE_OWNER_ALIAS', 'OPENED_DATE', 'SUPPORT_MISSION', 'PRODUCT_NAME', 'ACCOUNT_PRIORITY', 'OWNERSHIP_CHANGES', 'SEVERITY_LEVEL_NUMBER_FORMULA', 'ACCOUNT_NAME', 'BLUE_DIAMOND_ACCOUNT', 'PRODUCT_VERSION', 'OPERATING_SYSTEM', 'SUBJECT', 'CREATED_BY', 'LEGACY_PROBLEM_NUMBER', 'target']\nnearempty []\nexcludefromcolist ['CASE_NUMBER', 'target', 'OPENED_DATE']\ntextcols ['SUBJECT']\ncontinuouscols ['OWNERSHIP_CHANGES']\ncollist ['ACCOUNT_NAME', 'CASE_OWNER_ALIAS', 'SEVERITY_LEVEL_NUMBER_FORMULA', 'PRODUCT_NAME', 'BLUE_DIAMOND_ACCOUNT', 'LEGACY_PROBLEM_NUMBER', 'PRODUCT_VERSION', 'CREATED_BY', 'SUPPORT_MISSION', 'ACCOUNT_PRIORITY', 'OPERATING_SYSTEM']\nbefore mv\nprocessing  ACCOUNT_NAME\nprocessing  CASE_OWNER_ALIAS\nprocessing  SEVERITY_LEVEL_NUMBER_FORMULA\nprocessing  PRODUCT_NAME\nprocessing  BLUE_DIAMOND_ACCOUNT\nprocessing  LEGACY_PROBLEM_NUMBER\nprocessing  PRODUCT_VERSION\nprocessing  CREATED_BY\nprocessing  SUPPORT_MISSION\nprocessing  ACCOUNT_PRIORITY\nprocessing  OPERATING_SYSTEM\nprocessing text col SUBJECT\nusing loaded tokenizer\ncol in max is ACCOUNT_NAME\nmax is 2208\ni is  0\ncol in max is CASE_OWNER_ALIAS\nmax is 269\ni is  1\ncol in max is SEVERITY_LEVEL_NUMBER_FORMULA\nmax is 4\ni is  2\ncol in max is PRODUCT_NAME\nmax is 28\ni is  3\ncol in max is BLUE_DIAMOND_ACCOUNT\nmax is 2\ni is  4\ncol in max is LEGACY_PROBLEM_NUMBER\nmax is 2\ni is  5\ncol in max is PRODUCT_VERSION\nmax is 41\ni is  6\ncol in max is CREATED_BY\nmax is 2273\ni is  7\ncol in max is SUPPORT_MISSION\nmax is 1\ni is  8\ncol in max is ACCOUNT_PRIORITY\nmax is 19\ni is  9\ncol in max is OPERATING_SYSTEM\nmax is 6\ni is  10\ncol is SUBJECT\nmax is 5999\ntextmax 7000\nmax_dict {'PRODUCT_VERSION': 41, 'ACCOUNT_NAME': 2208, 'BLUE_DIAMOND_ACCOUNT': 2, 'CASE_OWNER_ALIAS': 269, 'SEVERITY_LEVEL_NUMBER_FORMULA': 4, 'OPERATING_SYSTEM': 6, 'CREATED_BY': 2273, 'SUPPORT_MISSION': 1, 'ACCOUNT_PRIORITY': 19, 'SUBJECT': 5999, 'PRODUCT_NAME': 28, 'LEGACY_PROBLEM_NUMBER': 2}\nzero target values: 1030\none target values: 11\ncat col is ACCOUNT_NAME\ncat col is CASE_OWNER_ALIAS\ncat col is SEVERITY_LEVEL_NUMBER_FORMULA\ncat col is PRODUCT_NAME\ncat col is BLUE_DIAMOND_ACCOUNT\ncat col is LEGACY_PROBLEM_NUMBER\ncat col is PRODUCT_VERSION\ncat col is CREATED_BY\ncat col is SUPPORT_MISSION\ncat col is ACCOUNT_PRIORITY\ncat col is OPERATING_SYSTEM\ntext col is SUBJECT\ncont col is OWNERSHIP_CHANGES\n"
                }
            ], 
            "source": "# main cell to invoke functions\n\n# get project token\ndef_project()\n# initialize switches\nbootstrap()\n# get data\ningest_data()\n# prep for unpickling of Keras model\nmake_keras_picklable()\n# load model, tokenizer, label encodings, and exclusion list\nloaded_model, tok_raw, lelist, nearempty = load_items()\n# build combined dataframe\nmerged_data, train = create_merged_df(dm_cases, merged_data)\nmerged_data.iloc[:,0:13].head()\n# define feature categories and ensure continuous cols have appropriate type\nallcols,textcols,continuouscols,collist = define_feature_categories(merged_data)\ntrain = fill_missing(train)\ntrain = encode_labels(train, lelist)\ntrain = encode_text(train, tok_raw)\n# get max values for embeddings\n# Sept 3 - don't define maxes - breaks creation of Keras variables\nmax_dict,textmax = get_max_embeddings(train)\ndtrain = train\n# print counts of target values for data sets\nprint(\"zero target values:\",(dtrain[\"target\"]==0).sum())\nprint(\"one target values:\",(dtrain[\"target\"]==1).sum())\n# X_train = get_keras_vars(dtrain)\nX_trainb, X_train_list = get_keras_vars(dtrain)\n\n# preds = loaded_model.predict(X_train_list, batch_size=BATCH_SIZE)"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nACCOUNT_NAME (InputLayer)       (None, 1)            0                                            \n__________________________________________________________________________________________________\nCASE_OWNER_ALIAS (InputLayer)   (None, 1)            0                                            \n__________________________________________________________________________________________________\nembedding_16 (Embedding)        (None, 1, 10)        22150       ACCOUNT_NAME[0][0]               \n__________________________________________________________________________________________________\nembedding_20 (Embedding)        (None, 1, 10)        2690        CASE_OWNER_ALIAS[0][0]           \n__________________________________________________________________________________________________\nSEVERITY_LEVEL_NUMBER_FORMULA ( (None, 1)            0                                            \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 1, 10)        40          embedding_16[0][0]               \n__________________________________________________________________________________________________\nbatch_normalization_20 (BatchNo (None, 1, 10)        40          embedding_20[0][0]               \n__________________________________________________________________________________________________\nembedding_13 (Embedding)        (None, 1, 10)        40          SEVERITY_LEVEL_NUMBER_FORMULA[0][\n__________________________________________________________________________________________________\nPRODUCT_NAME (InputLayer)       (None, 1)            0                                            \n__________________________________________________________________________________________________\nflatten_12 (Flatten)            (None, 10)           0           batch_normalization_16[0][0]     \n__________________________________________________________________________________________________\nflatten_13 (Flatten)            (None, 10)           0           batch_normalization_20[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 1, 10)        40          embedding_13[0][0]               \n__________________________________________________________________________________________________\nembedding_14 (Embedding)        (None, 1, 10)        280         PRODUCT_NAME[0][0]               \n__________________________________________________________________________________________________\nBLUE_DIAMOND_ACCOUNT (InputLaye (None, 1)            0                                            \n__________________________________________________________________________________________________\ndropout_14 (Dropout)            (None, 10)           0           flatten_12[0][0]                 \n__________________________________________________________________________________________________\ndropout_15 (Dropout)            (None, 10)           0           flatten_13[0][0]                 \n__________________________________________________________________________________________________\nflatten_14 (Flatten)            (None, 10)           0           batch_normalization_13[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 1, 10)        40          embedding_14[0][0]               \n__________________________________________________________________________________________________\nembedding_15 (Embedding)        (None, 1, 10)        20          BLUE_DIAMOND_ACCOUNT[0][0]       \n__________________________________________________________________________________________________\nLEGACY_PROBLEM_NUMBER (InputLay (None, 1)            0                                            \n__________________________________________________________________________________________________\nconcatenate_13 (Concatenate)    (None, 20)           0           dropout_14[0][0]                 \n                                                                 dropout_15[0][0]                 \n__________________________________________________________________________________________________\ndropout_16 (Dropout)            (None, 10)           0           flatten_14[0][0]                 \n__________________________________________________________________________________________________\nflatten_15 (Flatten)            (None, 10)           0           batch_normalization_14[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 1, 10)        40          embedding_15[0][0]               \n__________________________________________________________________________________________________\nembedding_17 (Embedding)        (None, 1, 10)        20          LEGACY_PROBLEM_NUMBER[0][0]      \n__________________________________________________________________________________________________\nPRODUCT_VERSION (InputLayer)    (None, 1)            0                                            \n__________________________________________________________________________________________________\nconcatenate_14 (Concatenate)    (None, 30)           0           concatenate_13[0][0]             \n                                                                 dropout_16[0][0]                 \n__________________________________________________________________________________________________\ndropout_17 (Dropout)            (None, 10)           0           flatten_15[0][0]                 \n__________________________________________________________________________________________________\nflatten_16 (Flatten)            (None, 10)           0           batch_normalization_15[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 1, 10)        40          embedding_17[0][0]               \n__________________________________________________________________________________________________\nembedding_18 (Embedding)        (None, 1, 10)        430         PRODUCT_VERSION[0][0]            \n__________________________________________________________________________________________________\nCREATED_BY (InputLayer)         (None, 1)            0                                            \n__________________________________________________________________________________________________\nconcatenate_15 (Concatenate)    (None, 40)           0           concatenate_14[0][0]             \n                                                                 dropout_17[0][0]                 \n__________________________________________________________________________________________________\ndropout_18 (Dropout)            (None, 10)           0           flatten_16[0][0]                 \n__________________________________________________________________________________________________\nflatten_17 (Flatten)            (None, 10)           0           batch_normalization_17[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_18 (BatchNo (None, 1, 10)        40          embedding_18[0][0]               \n__________________________________________________________________________________________________\nembedding_19 (Embedding)        (None, 1, 10)        22760       CREATED_BY[0][0]                 \n__________________________________________________________________________________________________\nSUPPORT_MISSION (InputLayer)    (None, 1)            0                                            \n__________________________________________________________________________________________________\nconcatenate_16 (Concatenate)    (None, 50)           0           concatenate_15[0][0]             \n                                                                 dropout_18[0][0]                 \n__________________________________________________________________________________________________\ndropout_19 (Dropout)            (None, 10)           0           flatten_17[0][0]                 \n__________________________________________________________________________________________________\nflatten_18 (Flatten)            (None, 10)           0           batch_normalization_18[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_19 (BatchNo (None, 1, 10)        40          embedding_19[0][0]               \n__________________________________________________________________________________________________\nembedding_21 (Embedding)        (None, 1, 10)        10          SUPPORT_MISSION[0][0]            \n__________________________________________________________________________________________________\nACCOUNT_PRIORITY (InputLayer)   (None, 1)            0                                            \n__________________________________________________________________________________________________\nconcatenate_17 (Concatenate)    (None, 60)           0           concatenate_16[0][0]             \n                                                                 dropout_19[0][0]                 \n__________________________________________________________________________________________________\ndropout_20 (Dropout)            (None, 10)           0           flatten_18[0][0]                 \n__________________________________________________________________________________________________\nflatten_19 (Flatten)            (None, 10)           0           batch_normalization_19[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_21 (BatchNo (None, 1, 10)        40          embedding_21[0][0]               \n__________________________________________________________________________________________________\nembedding_22 (Embedding)        (None, 1, 10)        190         ACCOUNT_PRIORITY[0][0]           \n__________________________________________________________________________________________________\nOPERATING_SYSTEM (InputLayer)   (None, 1)            0                                            \n__________________________________________________________________________________________________\nconcatenate_18 (Concatenate)    (None, 70)           0           concatenate_17[0][0]             \n                                                                 dropout_20[0][0]                 \n__________________________________________________________________________________________________\ndropout_21 (Dropout)            (None, 10)           0           flatten_19[0][0]                 \n__________________________________________________________________________________________________\nflatten_20 (Flatten)            (None, 10)           0           batch_normalization_21[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_22 (BatchNo (None, 1, 10)        40          embedding_22[0][0]               \n__________________________________________________________________________________________________\nembedding_23 (Embedding)        (None, 1, 10)        60          OPERATING_SYSTEM[0][0]           \n__________________________________________________________________________________________________\nSUBJECT (InputLayer)            (None, 5999)         0                                            \n__________________________________________________________________________________________________\nconcatenate_19 (Concatenate)    (None, 80)           0           concatenate_18[0][0]             \n                                                                 dropout_21[0][0]                 \n__________________________________________________________________________________________________\ndropout_22 (Dropout)            (None, 10)           0           flatten_20[0][0]                 \n__________________________________________________________________________________________________\nflatten_21 (Flatten)            (None, 10)           0           batch_normalization_22[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_23 (BatchNo (None, 1, 10)        40          embedding_23[0][0]               \n__________________________________________________________________________________________________\nembedding_24 (Embedding)        (None, 5999, 50)     350000      SUBJECT[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_20 (Concatenate)    (None, 90)           0           concatenate_19[0][0]             \n                                                                 dropout_22[0][0]                 \n__________________________________________________________________________________________________\ndropout_23 (Dropout)            (None, 10)           0           flatten_21[0][0]                 \n__________________________________________________________________________________________________\nflatten_22 (Flatten)            (None, 10)           0           batch_normalization_23[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_24 (BatchNo (None, 5999, 50)     200         embedding_24[0][0]               \n__________________________________________________________________________________________________\nconcatenate_21 (Concatenate)    (None, 100)          0           concatenate_20[0][0]             \n                                                                 dropout_23[0][0]                 \n__________________________________________________________________________________________________\ndropout_24 (Dropout)            (None, 10)           0           flatten_22[0][0]                 \n__________________________________________________________________________________________________\ngru_2 (GRU)                     (None, 16)           3216        batch_normalization_24[0][0]     \n__________________________________________________________________________________________________\nconcatenate_22 (Concatenate)    (None, 110)          0           concatenate_21[0][0]             \n                                                                 dropout_24[0][0]                 \n__________________________________________________________________________________________________\ndropout_13 (Dropout)            (None, 16)           0           gru_2[0][0]                      \n__________________________________________________________________________________________________\nconcatenate_23 (Concatenate)    (None, 126)          0           concatenate_22[0][0]             \n                                                                 dropout_13[0][0]                 \n__________________________________________________________________________________________________\nOWNERSHIP_CHANGES (InputLayer)  (None, 1)            0                                            \n__________________________________________________________________________________________________\nconcatenate_24 (Concatenate)    (None, 127)          0           concatenate_23[0][0]             \n                                                                 OWNERSHIP_CHANGES[0][0]          \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            128         concatenate_24[0][0]             \n==================================================================================================\nTotal params: 402,634\nTrainable params: 402,314\nNon-trainable params: 320\n__________________________________________________________________________________________________\n"
                }
            ], 
            "source": "loaded_model.summary()"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "InvalidArgumentError", 
                    "evalue": "indices[0,0] = 438 is not in [0, 269)\n\t [[Node: embedding_20/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_20/embeddings/read, embedding_20/Cast)]]\n\nCaused by op 'embedding_20/Gather', defined at:\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-3d0573bb4d89>\", line 12, in <module>\n    loaded_model, tok_raw, lelist, nearempty = load_items()\n  File \"<ipython-input-9-6bf05d6e7087>\", line 4, in load_items\n    loaded_model = pickle.load(project.get_file('pickledmodelaug4.pickle'))\n  File \"<ipython-input-8-fcc3dbede346>\", line 17, in __setstate__\n    model = keras.models.load_model(fd.name)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/models.py\", line 243, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/models.py\", line 317, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/layers/__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/utils/generic_utils.py\", line 144, in deserialize_keras_object\n    with CustomObjectScope(custom_objects):\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/engine/topology.py\", line 2520, in from_config\n    input_tensors = []\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/engine/topology.py\", line 2477, in process_node\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/layers/embeddings.py\", line 138, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1208, in gather\n    return tf.gather(reference, indices)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 2409, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1219, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[0,0] = 438 is not in [0, 269)\n\t [[Node: embedding_20/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_20/embeddings/read, embedding_20/Cast)]]\n", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[0,0] = 438 is not in [0, 269)\n\t [[Node: embedding_20/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_20/embeddings/read, embedding_20/Cast)]]", 
                        "\nDuring handling of the above exception, another exception occurred:\n", 
                        "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-21-3e19995b2035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1840\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             check_batch_axis=True)\n\u001b[1;32m   1844\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         num_samples = self._check_num_samples(ins, batch_size,\n\u001b[1;32m   1336\u001b[0m                                               \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m                                               'steps')\n\u001b[0m\u001b[1;32m   1338\u001b[0m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[0,0] = 438 is not in [0, 269)\n\t [[Node: embedding_20/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_20/embeddings/read, embedding_20/Cast)]]\n\nCaused by op 'embedding_20/Gather', defined at:\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-3d0573bb4d89>\", line 12, in <module>\n    loaded_model, tok_raw, lelist, nearempty = load_items()\n  File \"<ipython-input-9-6bf05d6e7087>\", line 4, in load_items\n    loaded_model = pickle.load(project.get_file('pickledmodelaug4.pickle'))\n  File \"<ipython-input-8-fcc3dbede346>\", line 17, in __setstate__\n    model = keras.models.load_model(fd.name)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/models.py\", line 243, in load_model\n    model = model_from_config(model_config, custom_objects=custom_objects)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/models.py\", line 317, in model_from_config\n    return layer_module.deserialize(config, custom_objects=custom_objects)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/layers/__init__.py\", line 55, in deserialize\n    printable_module_name='layer')\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/utils/generic_utils.py\", line 144, in deserialize_keras_object\n    with CustomObjectScope(custom_objects):\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/engine/topology.py\", line 2520, in from_config\n    input_tensors = []\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/engine/topology.py\", line 2477, in process_node\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/layers/embeddings.py\", line 138, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1208, in gather\n    return tf.gather(reference, indices)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 2409, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1219, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[0,0] = 438 is not in [0, 269)\n\t [[Node: embedding_20/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_20/embeddings/read, embedding_20/Cast)]]\n"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "preds = loaded_model.predict(X_train_list, batch_size=BATCH_SIZE)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(X_train_list)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(X_train_list)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "print(X_train_list)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "preds.shape"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "len(X_train_list)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "for i in X_train_list:\n    print(i.shape)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dtrain[\"predict\"] = preds\ndtrain.predict[:5]\nif verboseout:\n    dtrain.predict.hist()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get rounded predictions \ndtrain[\"predround\"] = preds.round().astype(int)\ndtrain.predround[:5]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get delta between predictions on training set and actual training target values\n# hand calculate accuracy on training set as ratio of (total training samples - wrong training predictions)/total training samples\n\ndeltatr = abs(dtrain.target[:100000] - dtrain.predround[:100000])\ndeltatr[:50]\nprint(deltatr.sum())\nprint(\"percentage correct train\")\nprint((len(deltatr) - deltatr.sum())/len(deltatr))"
        }, 
        {
            "source": "# Confusion matrix <a name='confusionmatrix' />\n<a href=#linkanchor>Back to link list</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn import metrics\n\ncfmap=metrics.confusion_matrix(y_true=dtrain['target'],  # True labels\n                         y_pred=dtrain[\"predround\"])\n\nlabel = [\"0\", \"1\"]\nsns.heatmap(cfmap, annot = True, xticklabels = label, yticklabels = label)\nplt.xlabel(\"Prediction\")\nplt.title(\"Confusion Matrix for DM prediction (weighted)\")\nplt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "type(X_train)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X_train"
        }, 
        {
            "source": "# Deploy model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import keras\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.datasets import mnist\nfrom keras.models import Sequential, load_model\n\nfrom keras import backend as K\nimport numpy as np\n\n'''\nbatch_size = 128\nnum_classes = 10\nepochs = 1\n\n# input shape\nimg_rows, img_cols = 28, 28\n\n# samples to train\nnum_train_samples = 500\n\nprint(K._backend)\n\n# prepare train and test datasets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n# normalize the data\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\nx_train = x_train[:num_train_samples]\ny_train = y_train[:num_train_samples]\nprint(x_train.shape)\nprint(y_train.shape)\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n# Model Definition\nX = Input(shape=(28, 28, 1))\nl1 = Conv2D(32, kernel_size=(3, 3),\n            activation='relu')(X)\nl2 = Conv2D(64, (3, 3), activation='relu')(l1)\nl21 = MaxPooling2D(pool_size=(2, 2))(l2)\nl22 = Dropout(0.25)(l21)\nl23 = Flatten()(l22)\n\nl3 = Dense(128, activation='relu')(l23)\nl31 = Dropout(0.5)(l3)\ny_hat = Dense(num_classes, activation='softmax', name=\"y_hat\")(l31)\n\nmodel = Model(inputs=X, outputs=y_hat)\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\n\n# Train a model\nmodel.fit(x=x_train,\n          y=y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(x_test, y_test))\nprint(\"model trained : \" + str(type(model)))\n# prediction_prob = model.predict(x_score)\n# print(prediction_prob)\n'''\n# Save model\nsave_path = \"dm_predict_modelj3c.h5\"\nloaded_model.save(save_path)\n\n"
        }, 
        {
            "source": "#### Create compressed archive of the saved model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ls -ltr dm_*"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!tar -zcvf dm_predict_modelj3c.h5.tgz dm_predict_modelj3c.h5"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "ls -ltr dm_*"
        }, 
        {
            "source": "## 2.0 Save the trained model to WML Repository", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Use `watson_machine_learning_client` Python library to save the trained model to WML Repository, to deploy the saved model and to make predictions using the deployed model.</br>\n\n\n`watson_machine_learning_client` can be installed using the following `pip` command:\n\n`!pip install watson-machine-learning-client --upgrade`", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!pip show watson-machine-learning-client "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "mkdir temp_install"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#!pip install watson-machine-learning-client --upgrade\n!pip install -b ./temp_install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.python.org/simple/ watson-machine-learning-client==1.0.133\n        "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client = WatsonMachineLearningAPIClient(wml_credentials)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import json"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model_props = {client.repository.ModelMetaNames.AUTHOR_NAME: \"IBM\", \n               # client.repository.ModelMetaNames.AUTHOR_EMAIL: \"ibm@ibm.com\", \n               client.repository.ModelMetaNames.AUTHOR_EMAIL: \"mryan@ca.ibm.com\", \n               client.repository.ModelMetaNames.NAME: \"DM_predict_kerasj3c\",\n               client.repository.ModelMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n               client.repository.ModelMetaNames.FRAMEWORK_VERSION: \"1.5\" ,\n               client.repository.ModelMetaNames.FRAMEWORK_LIBRARIES: [{\"name\": \"keras\", \"version\": \"2.1.3\"}]\n              }"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "!ls -ltr | grep dm"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# for model parameter, specify the .tgz file that you created with tar command\npublished_model = client.repository.store_model(model=\"dm_predict_modelj3c.h5.tgz\", meta_props=model_props)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import json"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "published_model_uid = client.repository.get_model_uid(published_model)\nmodel_details = client.repository.get_details(published_model_uid)\nprint(json.dumps(model_details, indent=2))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "### Delete the model from WML Repository\n### client.repository.delete(published_model_uid) "
        }, 
        {
            "source": "## 3.0 Deploy the Keras model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client.deployments.list()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# client.deployments.delete(\"0ddbe9e4-f538-46e3-abce-59c3d3620cc6\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "created_deployment = client.deployments.create(published_model_uid, name=\"dm_predict_kerasj3c\")\n# KK3_clt_keras_mnist_mark"
        }, 
        {
            "source": "## 4.0 Predict using the deployed model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "scoring_endpoint = client.deployments.get_scoring_url(created_deployment)\n\nprint(scoring_endpoint)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# X_train is a dictionary - for each element the value is a large array of values\n# X_train = get_keras_vars(dtrain)\n\n'''\n for col in collist:\n        X[col] = np.array(dataset[col])\n        if verboseout:\n            print(\"cat col is\",col)\n            print(\"shape is\",X[col].shape)\n  \n  Result is:\n            \ncat col is LEGACY_PROBLEM_NUMBER\nshape is (1041,)\ncat col is ACCOUNT_NAME\nshape is (1041,)\ncat col is CREATED_BY\nshape is (1041,)\ncat col is ACCOUNT_PRIORITY\nshape is (1041,)\ncat col is SEVERITY_LEVEL_NUMBER_FORMULA\nshape is (1041,)\ncat col is PRODUCT_VERSION\nshape is (1041,)\ncat col is PRODUCT_NAME\nshape is (1041,)\ncat col is CASE_OWNER_ALIAS\nshape is (1041,)\ncat col is SUPPORT_MISSION\nshape is (1041,)\ncat col is BLUE_DIAMOND_ACCOUNT\nshape is (1041,)\ncat col is OPERATING_SYSTEM\nshape is (1041,)\ncont col is OWNERSHIP_CHANGES\nshape is (1041,)\ntext col is SUBJECT\nshape is (1041, 5999)'''"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X_train[\"LEGACY_PROBLEM_NUMBER\"][4]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "'''\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nmnist.load_data() returns Tuple of Numpy arrays: (x_train, y_train), (x_test, y_test)\ntype(x_test): numpy.ndarray\nx_test.shape: (10000, 28, 28, 1)\nx_test[23].shape: (28, 28, 1)\ntype(x_test[23]): numpy.ndarray\ntype(x_score_1): list\n\nand from score documentation:\n>>> scoring_payload = {'fields': ['GENDER','AGE','MARITAL_STATUS','PROFESSION'], 'values': [['M',23,'Single','Student'],['M',55,'Single','Executive']]}\n>>> predictions = client.deployments.score(scoring_url, scoring_payload)\n\n\nx_score_1 = x_test[23].tolist()\nx_score_2 = x_test[32].tolist()\n'''\n# from own code this is how model gets invoked\n# preds = loaded_model.predict(X_train, batch_size=BATCH_SIZE)\n# x_score_1 = X_train[23].tolist()\n# x_score_2 = X_train[32].tolist()\n# scoring_payload = {'values': [x_score_1, x_score_2]}\n# try to create a list for items for\ni = 0\nlister = []\nfor col in collist:\n    # L.append(obj)\n    # X_train[\"LEGACY_PROBLEM_NUMBER\"][4]\n    lister.append(X_train[col][i])\n        \nfor col in continuouscols:\n    lister.append(X_train[col][i])\n            \n    \nfor col in textcols:\n    lister.append(X_train[col][i])\n\n\nscoring_payload = {'values': lister}"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "lister"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "predictions = client.deployments.score(scoring_endpoint, scoring_payload)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(json.dumps(predictions, indent=2))"
        }, 
        {
            "source": "### Delete Deployment\nPlease ensure to delete the Keras deployments if they are no longer required. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client.deployments.delete(client.deployments.get_uid(created_deployment))"
        }, 
        {
            "source": "# Kaggle submission that was used as input for this notebook\nhttps://www.kaggle.com/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Summary\nThis notebook shows loading and deployment of a Keras model for predicting Duty Manager calls.\n\n# Author\n\nMark Ryan is a manager at IBM Canada.\n\nCopyright \u00a9 IBM Corp. 2018. This notebook and its source code are released under the terms of the MIT License.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}