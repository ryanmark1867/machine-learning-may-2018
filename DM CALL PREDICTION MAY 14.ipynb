{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Applying Deep Learning to solve a Client Success problem - Duty Manager call prediction\n\nGOAL: tackle a day-to-day Db2 Client Success problem using deep learning\n\nPROBLEM CHOSEN: predict likelihood a ticket will generate a duty manager call\n\nDATA SOURCE: ticketing dashboard (ticket details) \n\nFEATURES: TBD\n    \n\n\nAPPROACH: start with a basic, working DL model and simplify/augment to optimizer prediction accuracy\n- using this article on structured DL (i.e. DL on structured data, as opposed to common DL examples on unstructured data like images & audio) https://towardsdatascience.com/structured-deep-learning-b8ca4138b848 as a starting point, look for some examples of end-to-end DL on structured data. This article refers to a Kaggle competition on structured data, an entry in which https://www.kaggle.com/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl , for example, is a great end-to-end example\n- use Keras (Python DL framework built on top of TensorFlow) - sweet spot for a simple, working DL model. Keras has decent documentation and a big enough community that answers to many questions are readily available\n- once input data working through model, tune the following parameters to optimize validation/test accuracy (accuracy of prediction on data not used to generate model weights):\n- learning rate (to avoid local minima and oscillation)\n- dropout rate (control overfitting by randomly dropping nodes)\n- lambda for L2 regularization (control overfitting by \"suppressing\" large weights)\n- batch size (the number of training examples in a forward/backward pass)\n- epochs run (number of complete passes through dataset)\n\nENVIRONMENTS USED:\n- public cloud DSX: for most development and testing after initial setup and prior to adding non-Db2 data. PROS: convenient, persistent sessions, no cost; CONS: performance ~ 30% slower than PaperSpace, memory errors trying to run model on larger data set including text analysis\n- Paperspace (https://www.paperspace.com/) low-cost generic GPU-enabled Linux environemnt. Used instructions from Jeremy Howard fast.ai course V2 to setup. PROS: better perf, able to handle text processing w. largest data set without memory errors, standard filesystem; CONS: setup not straightforward, inconvenience of having to start & stop environment\n- initial development done on Paperspace, moved to DSX once the basics were working, then back to Paperspace after the larger data set began to generate memory errors in DSX\n\nSOURCE DATA :\n\nOVERVIEW OF CODE:\n- ingest CSVs: need distinct code for DSX and paper\n- various cleansing / slicing and dicing to generate additional features (e.g. year and day of week from date opened). Several variations attempted (e.g. make severity a continuous variable, distort severity at extremes, exclude some severities, exclude some TTR ranges) \n- define label / target: new column in dataframe that is 0 or 1 depending on whether TTR is less than or more than 1 day\n- split data into training (which will be further split into training/validation) and test data sets (training data used to generate model weights; validation set used to track how well model predicts actual targets as hyperparameters are tuned; test set used to validate entire model (learned weights and hyperparameters)\n- fill in missing values for all features\n- in categorical columns (all feature columns currently) replace values (e.g. \"CANADA\", \"JAPAN\" for Country) with integer IDs\n- process text columns - replace words with vector of integer IDs\n- define Keras model:\n- embedding layers for all categorical features (embeddings are vectors used to learn patterns for categorical data - https://en.wikipedia.org/wiki/Word_embedding )\n- RNN layer for text feature\n- single dense output layer w. sigmoid activation to output TTR prediction\n\nCURRENT HWMs:\n\nVARIATIONS ATTEMPTED:\n\nDL Glossary: https://deeplearning4j.org/glossary", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# set overall parameters\n\ndsxmode = True # overall is this being run in DSX?\ncsvmode = True # ingest from CSV\ndbmode = False # ingest from database\n\ntestproportion = 0.01 # proportion of data reserved for test set\ntrainproportion = 0.8 # proportion of non-test data dedicated to training (vs. validation)\nverboseout = False\nincludetext = True # switch to determine whether text fields are included in model\ntargetthresh = 6.0\nemptythresh = 1000\n# to address imbalance in training data between zero (above targetthresh) and detractor (below targetthresh) specify weight in compile and fit\n# class_weight = {0 : zero_weight, 1: one_weight}\n# consider calculating these values from actual skew rather than hard-coding them here\nzero_weight = 1.0\none_weight = 4.5\n\n# hyperparameters\nlearning_rate = 0.001\ndropout_rate = 0.00003 #0.003\nl2_lambda = 0.00003 #7.5\n"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# common imports\nimport zipfile\nimport pandas as pd\nimport numpy as np\nimport time\nimport datetime\nfrom dateutil import relativedelta\nfrom io import StringIO\nimport pandas as pd\n\n# DSX code to import uploaded documents\nfrom io import StringIO\nimport requests\nimport json\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.cross_validation import train_test_split\n\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport math\nfrom subprocess import check_output\nimport seaborn as sns\n"
        }, 
        {
            "source": "# Data input\n\nRead in data from external source:\n- DSX - access data from DWoC table\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 8, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(96, 14)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dm_cases.shape"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 9, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(7086, 9)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "merged_data.shape"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 10, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Case_Number</th>\n      <th>Subject</th>\n      <th>Owner_Last_Name</th>\n      <th>Account_Name_Text</th>\n      <th>Contact_Name</th>\n      <th>Account_Priority</th>\n      <th>Legacy_Problem_Number</th>\n      <th>Severity_Level</th>\n      <th>Case_Owner_Alias</th>\n      <th>Blue_Diamond_Account</th>\n      <th>Country</th>\n      <th>Watson_Skill_Suggestion_(Deprecated)</th>\n      <th>Status</th>\n      <th>Date_Opened</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TS000003163</td>\n      <td>FMP exiting with error and HeadExtentID 260611...</td>\n      <td>Ma</td>\n      <td>BELASTINGDIENST CAO</td>\n      <td>Vannisselroy, Koen</td>\n      <td>Analytics-GEP</td>\n      <td>28,585,211,788</td>\n      <td>3 - Minor business impact</td>\n      <td>majason</td>\n      <td>No</td>\n      <td>NL</td>\n      <td>NaN</td>\n      <td>Closed by IBM</td>\n      <td>08/11/2017</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TS000003166</td>\n      <td>W R db2 load process hang for loading 5.8T rec...</td>\n      <td>BADIGER</td>\n      <td>CA Franchise Tax Board</td>\n      <td>Jiang, Lily</td>\n      <td>NaN</td>\n      <td>65,923,227,000</td>\n      <td>2 - Significant impact (any system is down)</td>\n      <td>sbad</td>\n      <td>No</td>\n      <td>US</td>\n      <td>NaN</td>\n      <td>Closed - Archived</td>\n      <td>09/06/2017</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TS000007186</td>\n      <td>UPDATE SYSSTAT.INDEXES hangs</td>\n      <td>Koranteng-Asante</td>\n      <td>Lloyds Banking Group Service Delivery, Group IT</td>\n      <td>Nelson, Philip</td>\n      <td>NaN</td>\n      <td>Not Applicable</td>\n      <td>2 - Significant impact (any system is down)</td>\n      <td>asan</td>\n      <td>No</td>\n      <td>GB</td>\n      <td>NaN</td>\n      <td>Closed - Archived</td>\n      <td>9/21/2017</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TS000007630</td>\n      <td>Notified Load operation fails after revoking P...</td>\n      <td>Koranteng-Asante</td>\n      <td>STATE OF CONNECTICUT</td>\n      <td>McCabe, Barbara</td>\n      <td>Analytics-Industry</td>\n      <td>Not Applicable</td>\n      <td>2 - Significant impact (any system is down)</td>\n      <td>asan</td>\n      <td>No</td>\n      <td>US</td>\n      <td>NaN</td>\n      <td>Waiting for IBM</td>\n      <td>9/26/2017</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TS000027965</td>\n      <td>Sporadic SQL SP slow performance due to invali...</td>\n      <td>Ma</td>\n      <td>21ST CENTURY INSURANCE AND FINANCIAL SERVICES</td>\n      <td>Pentyala, Dileep Kumar</td>\n      <td>Analytics-Industry</td>\n      <td>Not Applicable</td>\n      <td>2 - Significant impact (any system is down)</td>\n      <td>majason</td>\n      <td>No</td>\n      <td>US</td>\n      <td>NaN</td>\n      <td>Waiting for IBM</td>\n      <td>11/01/2017</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   Case_Number                                            Subject  \\\n0  TS000003163  FMP exiting with error and HeadExtentID 260611...   \n1  TS000003166  W R db2 load process hang for loading 5.8T rec...   \n2  TS000007186                       UPDATE SYSSTAT.INDEXES hangs   \n3  TS000007630  Notified Load operation fails after revoking P...   \n4  TS000027965  Sporadic SQL SP slow performance due to invali...   \n\n    Owner_Last_Name                                Account_Name_Text  \\\n0                Ma                              BELASTINGDIENST CAO   \n1           BADIGER                           CA Franchise Tax Board   \n2  Koranteng-Asante  Lloyds Banking Group Service Delivery, Group IT   \n3  Koranteng-Asante                             STATE OF CONNECTICUT   \n4                Ma    21ST CENTURY INSURANCE AND FINANCIAL SERVICES   \n\n             Contact_Name    Account_Priority Legacy_Problem_Number  \\\n0      Vannisselroy, Koen       Analytics-GEP        28,585,211,788   \n1             Jiang, Lily                 NaN        65,923,227,000   \n2          Nelson, Philip                 NaN        Not Applicable   \n3         McCabe, Barbara  Analytics-Industry        Not Applicable   \n4  Pentyala, Dileep Kumar  Analytics-Industry        Not Applicable   \n\n                                Severity_Level Case_Owner_Alias  \\\n0                    3 - Minor business impact          majason   \n1  2 - Significant impact (any system is down)             sbad   \n2  2 - Significant impact (any system is down)             asan   \n3  2 - Significant impact (any system is down)             asan   \n4  2 - Significant impact (any system is down)          majason   \n\n  Blue_Diamond_Account Country Watson_Skill_Suggestion_(Deprecated)  \\\n0                   No      NL                                  NaN   \n1                   No      US                                  NaN   \n2                   No      GB                                  NaN   \n3                   No      US                                  NaN   \n4                   No      US                                  NaN   \n\n              Status Date_Opened  target  \n0      Closed by IBM  08/11/2017       1  \n1  Closed - Archived  09/06/2017       1  \n2  Closed - Archived   9/21/2017       1  \n3    Waiting for IBM   9/26/2017       1  \n4    Waiting for IBM  11/01/2017       1  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# columns = ['Col1', 'Col2', ...]\n# df.drop(columns, inplace=True, axis=1)\ndm_cases[\"target\"] = 1\n\ndm_cases.head()"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 11, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "['Contact_Name',\n 'Legacy_Problem_Number',\n 'Blue_Diamond_Account',\n 'Subject',\n 'Country',\n 'Case_Number',\n 'Case_Owner_Alias',\n 'Owner_Last_Name',\n 'Account_Name_Text',\n 'Date_Opened',\n 'Account_Priority',\n 'Watson_Skill_Suggestion_(Deprecated)',\n 'Severity_Level',\n 'Status']"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dmcollist = list(dm_cases)\ndmexlist = ['CASE_NUMBER','target']\n# collist = list(set(nontextcols) - set(excludefromcolist) - set(nearempty))\ndmcollist = list(set(dmcollist) - set(dmexlist))\ndmcollist"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# df.drop(columns, inplace=True, axis=1)\ndm_cases.drop(dmcollist, inplace=True, axis=1)"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 13, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "1    96\nName: target, dtype: int64"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dm_cases.target.value_counts()"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 14, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(96, 1)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dm_cases.shape"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "KeyError", 
                    "evalue": "'CASE_NUMBER'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2521\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;31mKeyError\u001b[0m: 'CASE_NUMBER'", 
                        "\nDuring handling of the above exception, another exception occurred:\n", 
                        "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-15-69e7706efc8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pd.merge(restaurant_ids_dataframe, restaurant_review_frame,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# on='business_id', how='outer')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdm_cases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'CASE_NUMBER'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     55\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                          validate=validate)\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    563\u001b[0m         (self.left_join_keys,\n\u001b[1;32m    564\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3837\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3838\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3839\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3840\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2522\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n", 
                        "\u001b[0;31mKeyError\u001b[0m: 'CASE_NUMBER'"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "# pd.merge(restaurant_ids_dataframe, restaurant_review_frame, \n# on='business_id', how='outer')\nmerged_data = pd.merge(merged_data,dm_cases,on= 'CASE_NUMBER', how='outer')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "merged_data.target.value_counts()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# df['column']=df['column'].fillna(value)\nmerged_data['target']=merged_data['target'].fillna(0.0)\nmerged_data.target.value_counts()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "merged_data.shape"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "merged_data.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(\"md shape\",merged_data.shape)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sns.heatmap(merged_data.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "nearempty = []\nfor c in merged_data.columns:\n    if np.sum(merged_data[c].isnull()) >= emptythresh :\n        print(c)\n        nearempty.append(c)\nprint(nearempty)"
        }, 
        {
            "source": "# Clean up data\n\nGo through steps to prepare data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Define test / training sets; fill in missing values; encode categorical values; process text field", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get training and test data set\n# test_size was 0.3 up to Jan 22\n\ntrain, test = train_test_split(merged_data, test_size = testproportion)\nprint(\"Through train test split. Test proportion:\")\nprint(testproportion)\nprint(\"zero target values\",(merged_data[\"target\"]==0).sum())\nprint(\"one target values\",(merged_data[\"target\"]==1).sum())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "allcols = list(merged_data)\nprint(\"all cols\",allcols)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# define the required column lists\n#textcols = ['Other_Reason_for_Cancellation', 'Reason_for_Reopening', 'Resolution_Description', 'Subject']\ntextcols = ['SUBJECT']\nexcludefromcolist = ['CASE_NUMBER','target']\n# list(set(temp1) - set(temp2))\nnontextcols = list(set(allcols) - set(textcols))\ncollist = list(set(nontextcols) - set(excludefromcolist) - set(nearempty))\n\n# print column list lengths:\nprint(\"allcols\",len(allcols))\nprint(\"nearempty\",len(nearempty))\nprint(\"textcols\",len(textcols))\nprint(\"collist\",len(collist))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\n\nprint(\"testo\")\nprint(\"length collist\",len(collist))\nprint(\"collist\",collist)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# fill missing values\nprint(\"before mv\")\ndef fill_missing(dataset,collist):\n    for col in collist:\n        dataset[col].fillna(value=\"missing\", inplace=True)\n    return (dataset)\n\ntrain = fill_missing(train,allcols)\ntest = fill_missing(test,allcols)\ntrain.head(3)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# process categorical data\n# replace values with category IDs in the following columns\n# \n\n\nle = LabelEncoder()\n\nfor col in collist:\n    if verboseout:\n        print(\"processing \",col)\n    le.fit(np.hstack([train[col], test[col]]))\n    train[col] = le.transform(train[col])\n    test[col] = le.transform(test[col])\n    \ndel le\n\n\ntrain.head(5)\n    "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "train['SUBJECT'].head(3)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# part 1 of text processing\n# tokenize list of text columns (made up of multiple strings)\n\nfrom keras.preprocessing.text import Tokenizer\n\n# text columns that we care about\n# textcols = ['Other_Reason_for_Cancellation', 'Reason_for_Reopening', 'Resolution_Description', 'Subject']\n# textcols = ['Resolution_Description', 'Subject']\n\nfor col in textcols:\n    if verboseout:\n        print(\"processing text col\",col)\n    raw_text = train[col].str.lower()\n    tok_raw = Tokenizer()\n    tok_raw.fit_on_texts(raw_text)\n    train[col] = tok_raw.texts_to_sequences(train[col].str.lower())\n    test[col] = tok_raw.texts_to_sequences(test[col].str.lower())\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "train['SUBJECT'].head(10)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# textcols = ['Other_Reason_for_Cancellation', 'Reason_for_Reopening', 'Resolution_Description', 'Subject']\n'''\nprint(\"res decr max\",train['Resolution_Description'].max())\nprint(\"subject\",train['SUBJECT'].max())\nprint(\"orc\",train['Other_Reason_for_Cancellation'].max())\nprint(\"Reason_for_Reopening\",train['Reason_for_Reopening'].max())\nprint(\"cmr number max\",train['CMR_Number'].max())\nprint(\"npmaxer \",np.max([np.max(train['Resolution_Description'].max()), np.max(train['Subject'].max()),np.max(train['Other_Reason_for_Cancellation'].max()),np.max(train['Reason_for_Reopening'].max())])) \nprint(\"test npmaxer \",np.max([np.max(test['Resolution_Description'].max()), np.max(test['Subject'].max()),np.max(test['Other_Reason_for_Cancellation'].max()),np.max(test['Reason_for_Reopening'].max())])) \n#print(\"overall npmaxer \",np.max([np.max(merged_data['Resolution_Description'].max()), np.max(merged_data['Subject'].max()),np.max(merged_data['Other_Reason_for_Cancellation'].max()),np.max(merged_data['Reason_for_Reopening'].max())]))\n'''"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# part 2 of text processing\n# max_abstract_seq = np.max([np.max(train.seq_abstract.apply(lambda x: len(x))), np.max(test.seq_abstract.apply(lambda x: len(x)))])\n\n# print(\"max name seq \"+str(max_abstract_seq))\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# max values for embeddings\n\n\n\nmax_dict = {}\ntextmax = 50\n\nfor col in collist:\n    max_dict[col] = np.max([train[col].max(), test[col].max()])+1\n    \n# np.max([np.max(train['Resolution_Description'].max()), np.max(train['Subject'].max()),np.max(train['Other_Reason_for_Cancellation'].max()),np.max(train['Reason_for_Reopening'].max())])) \nfor cols in textcols:\n    max_dict[cols] = np.max([np.max(train[cols].max()), np.max(test[cols].max())])+10\n    if max_dict[cols] > textmax:\n        textmax = max_dict[cols]\n\nprint(\"textmax\",textmax)\n                             \nif verboseout:\n    print(\"max_dict\",max_dict)\n\n"
        }, 
        {
            "source": "# Split training set into train / validate", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# define and scale target and get validation sets\n# apar_ds['TTRgtthresh'] = np.where(apar_ds['Time_to_relief'] >= ttrthresh,1,0)\n\n# train[\"target\"] = np.where(traintrain[\"LIKELIHOOD_TO_RECOMMEND\"]\n\n# train[\"target\"] = train[\"Time_to_relief\"]\n\n#train[\"target\"] = np.log(train.Time_to_relief+1)\n#target_scaler = MinMaxScaler(feature_range=(-1, 1))\n#train[\"target\"] = target_scaler.fit_transform(train.target.reshape(-1,1))\n#pd.DataFrame(train.target).hist()\n\n# train_size 0.8 up to Jan 22\ndtrain, dvalid = train_test_split(train, random_state=123, train_size=trainproportion)\n\n# modelfit = model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n#          , validation_data=(X_valid, dvalid.target), verbose=1)\n\nprint(dtrain.shape)\nprint(dvalid.shape)\nif verboseout:\n    train[\"target\"].head(10)\nelse:\n    #trn_labels[:4]\n    print(dtrain[\"target\"][:4])\n    print(dvalid.target[:4])\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# define keras variables\nfrom keras.preprocessing.sequence import pad_sequences\n\n# 'Country','Release','Comp_name','Dmcall','Severity','English_Spoken','Owner_ID','BE_Indicator','Customer','Technical_Resolver_ID'\n# MAX_COUNTRY ,MAX_RELEASE ,MAX_COMP_NAME ,MAX_DMCALL ,MAX_SEVERITY ,MAX_ENGLISH_SPOKEN ,MAX_OWNER_ID ,MAX_BE_INDICATOR ,MAX_CUSTOMER ,MAX_TECHNICAL_RESOLVER_ID \n# FOLLOW UP - put this in a loop so it's not hard coded\n\n# X for the features used\n\ndef get_keras_vars(dataset):\n    X = {}\n    for col in collist:\n        if verboseout:\n            print(\"cat col is\",col)\n        X[col] = np.array(dataset[col])\n   \n    for col in textcols:\n        if verboseout:\n            print(\"text col is\",col)\n        X[col] = pad_sequences(dataset[col], maxlen=max_dict[col])\n        \n    return X\n\nX_train = get_keras_vars(dtrain)\nX_valid = get_keras_vars(dvalid)\nX_test = get_keras_vars(test)\nprint(\"keras variables defined\")\n\n\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# shortcut for quick hyperparameter tweaks\n# l2_lambda = 7.5\n# dropout_rate = 0.03\n# learning_rate = 0.001\n# textcols = ['Other_Reason_for_Cancellation', 'Reason_for_Reopening', 'Resolution_Description', 'Subject']\n\n\ndropout_rate = 0.0003\nl2_lambda = 0.0003"
        }, 
        {
            "source": "# Define & run model\n\nIdentify model input, layers, and compilation details. \n\nRun model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# define model in keras\n\n# basic imports\nfrom keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import regularizers\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras import backend as K\nfrom keras.utils.vis_utils import plot_model\n\ndef onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())\n\n# original lambda l2_lambda = 0.0001\n# for 5 x increase in dataset, increase lambda 5x\n\n\ndef get_callbacks(filepath, patience=2):\n    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [es, msave]\n\n\ndef get_model():\n    \n   \n    #Inputs\n    # name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    #Prob_Abstract_Text = Input(shape=[X_train[\"Prob_Abstract_Text\"].shape[1]], name=\"Prob_Abstract_Text\")\n    # Country = Input(shape=[1], name=\"Country\")\n    # Release = Input(shape=[1], name=\"Release\")\n    #Release_Linear = Input(shape=[1], name=\"Release_Linear\")\n    #Report_Date_days = Input(shape=[1], name=\"Report_Date_days\")\n    \n    catinputs = {}\n    textinputs = {}\n    embeddings = {}\n    textembeddings = {}\n    catemb = 10\n    textemb = 50\n    \n    print(\"about to define embeddings\")\n    \n    print(\"about to define embeddings\")\n    collistfix = []\n    textlayerlist = []\n    inputlayerlist = []\n    i = 0\n    print(\"textmax is\",textmax)\n    for col in collist:\n        catinputs[col] = Input(shape=[1],name=col)\n        inputlayerlist.append(catinputs[col])\n        #print(\"inputname\",inputname)\n        #print(\"type inputname\",type(inputname))\n        #print(\"catinputs[col] type\", type(catinputs[col]))\n        #catinputs[col] = Input(shape=[1],name=col)\n        embeddings[col] = (Embedding(max_dict[col],catemb) (catinputs[col]))\n        # batchnorm all \n        embeddings[col] = (BatchNormalization() (embeddings[col]))\n        collistfix.append(embeddings[col])\n        \n    if includetext:    \n        for col in textcols:\n            print(\"col\",col)\n            textinputs[col] = Input(shape=[X_train[col].shape[1]], name=col)\n            print(\"text input shape\",X_train[col].shape[1])\n            inputlayerlist.append(textinputs[col])\n            textembeddings[col] = (Embedding(textmax,textemb) (textinputs[col]))\n            textembeddings[col] = (BatchNormalization() (textembeddings[col])) \n            textembeddings[col] = Dropout(dropout_rate) ( GRU(16,kernel_regularizer=l2(l2_lambda)) (textembeddings[col]))\n            collistfix.append(textembeddings[col])\n            print(\"max in the midst\",np.max([np.max(train[col].max()), np.max(test[col].max())])+10)\n        print(\"through loops for cols\")\n        #textcols = ['Other_Reason_for_Cancellation', 'Reason_for_Reopening', 'Resolution_Description', 'Subject']\n        \n    \n    \n    #rnn layer here if there were text fields\n        \n    # rnn_layer1 = Dropout(dropout_rate) ( GRU(16,kernel_regularizer=l2(l2_lambda)) (emb_Abstract))\n    # rnn_layer2 = GRU(8) (emb_name)\n    \n    #concatlist = []\n    #for cols in collist:\n    #    concatlist = concatlist+(Dropout(dropout_rate) (Flatten() (embeddings[cols])))\n    #print(\"concatlist\",concatlist)\n    # 'ACCOUNT_NAME', 'CASE_OWNER_ALIAS'\n          \n    # main_l = concatenate([\n    main_l = concatenate([Dropout(dropout_rate) (Flatten() (embeddings['ACCOUNT_NAME']) ),Dropout(dropout_rate) (Flatten() (embeddings['CASE_OWNER_ALIAS']) )])\n    for cols in collist:\n        if (cols != 'ACCOUNT_NAME') & (cols != 'CASE_OWNER_ALIAS'):\n            main_l = concatenate([main_l,Dropout(dropout_rate) (Flatten() (embeddings[cols]) )])\n    \n    print(\"through definition of non-text parts of main_l\")\n    if includetext:\n        for col in textcols:\n            main_l = concatenate([main_l,textembeddings[col]])\n                                                 \n                                                 \n    print(\"main_l\", main_l)                                            \n    \n    # originally 2 dense layers here, sizes were 128 / 64 \n    #main_l2 = Dropout(dr_r) (Dense(128,kernel_regularizer=l2(l2_lambda)) (main_l))\n    # main_l3 = Dropout(dropout_rate) (Dense(32,kernel_regularizer=l2(l2_lambda)) (main_l))\n    \n    #output\n    # change to softmax for TTRthreshold\n    #output = Dense(1, activation=\"linear\") (main_l)\n    #output = Dense(1, activation=\"softmax\") (main_l)\n    \n    \n    #output = Dense(1, activation=\"sigmoid\") (main_l)\n    output = Dense(1, activation=\"sigmoid\") (main_l)\n    # output = Dense(1, activation=\"linear\") (main_l)\n    \n    #model\n        \n    # model = Model([Country,Release,Comp_name,Initial_Severity_Linear, Owner_ID,Customer,Technical_Resolver_ID, System_Down_Indicator, Prob_Abstract_Text], output)\n    # model = Model([Country,Release,Product,Initial_Severity_Linear, Owner_ID,Customer,Technical_Resolver_ID, Prob_Abstract_Text], output)\n    #model = Model([Country,Release_Linear,Product,Initial_Severity_Linear, Owner_ID,Customer,Technical_Resolver_ID, Prob_Abstract_Text], output)\n    # [Country, Report_Date_days,Comp_name,Initial_Severity_Linear, Owner_ID,Customer,Technical_Resolver_ID, Prob_Abstract_Text, Release_Linear]\n                                                   \n    model = Model(inputlayerlist, output)\n    \n    # model = Model([Country,Release,Product,Initial_Severity_Linear,Customer,Technical_Resolver_ID, Prob_Abstract_Text], output)\n    # optimizer = SGD(lr=0.01, momentum=0.4)\n    \n    \n    # optimizer = Adam(lr=learning_rate)\n    optimizer = SGD(lr=learning_rate)\n   \n    # optimizer = SGD(lr=0.5)\n    # model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"], weighted_metrics=[\"accuracy\"])\n    # model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n    \n      \n    \n    return model\n\n    \nmodel = get_model()\nif dsxmode == False:\n    plot_model(model, to_file='/home/paperspace/visualizations/model_plotfeb15.png', show_shapes=True, show_layer_names=True)\nmodel.summary()\n    \n\n    "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "BATCH_SIZE = 200\nepochs = 10\nprint(\"text cols\",textcols)\nprint(\"dropout \",dropout_rate)\nprint(\"L2 lambda \",l2_lambda)\nprint(\"batch size \",BATCH_SIZE)\n\n\nmodel = get_model()\nmodelfit = model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n         , validation_data=(X_valid, dvalid.target),class_weight = {0 : zero_weight, 1: one_weight}, verbose=1)\n# modelfit = model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n#           , validation_data=(X_valid, dvalid.target), verbose=1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "if dsxmode == False:\n    model.save('/home/paperspace/models/TTRmodelfeb15b.h5')\n# model = load_model('my_model.h5')"
        }, 
        {
            "source": "# Predictions and renderings", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# functions to parse and manipulate dates in the style of the input CSVs\nimport datetime\nfrom datetime import date\ndef create_date(year,month):\n    outdate = datetime.date(year,month,15)\n    return(outdate)\n\ndef parse_bic_date(bic_date_in):\n    year = int(bic_date_in[0:4])\n    month = int(bic_date_in[-2:])\n    return(year,month)\n\ndef create_date_from_bic(bic_date_in):\n    yr,mth = parse_bic_date(bic_date_in)\n    retdate = create_date(yr,mth)\n    return retdate\n\ndef get_datecomp_from_csvdate (csv_date):\n    day = int(csv_date[0:2])\n    # month_number = datetime.datetime.strptime(month_name, '%b').month\n    month = datetime.datetime.strptime(csv_date[3:6], '%b').month\n    year = int('20'+csv_date[-2:])\n    # year = int(csv_date[-2:])\n    return (year,month,day)\n\ndef get_date_from_csvdate (csv_date):\n    day = int(csv_date[0:2])\n    # month_number = datetime.datetime.strptime(month_name, '%b').month\n    month = datetime.datetime.strptime(csv_date[3:6], '%b',coerce=True).month\n    year = int('20'+csv_date[-2:])\n    # year = int(csv_date[-2:])\n    return (date(year,month,day))\n\ndef get_year_from_csvdate (csv_date):\n    year = int('20'+csv_date[-2:])\n    return (year)\n\ndef get_month_from_csvdate (csv_date):\n    month = datetime.datetime.strptime(csv_date[3:6], '%b',coerce=True).month\n    return (month)\n\ndef get_day_from_csvdate (csv_date):\n    day = int(csv_date[0:2])\n    return (day)\n\ndef get_weekday (date):\n    return(date.weekday())\n\n# pd.to_datetime(x, coerce=True)\n\ndef validatedate(csv_text):\n    try:\n        datetime.datetime.strptime(csv_date[3:6], '%b')\n    except ValueError:\n        raise ValueError(\"Incorrect data format, should be YYYY-MM-DD\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# predictions on training set\n\npreds = model.predict(X_train, batch_size=BATCH_SIZE)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "preds[:50]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dtrain[\"predict\"] = preds\ndtrain.predict[:5]\nif verboseout:\n    dtrain.predict.hist()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "if verboseout:\n    dtrain.predict.hist(bins=2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "if verboseout:\n    dvalid.predict.hist(bins=2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# print(type(apar_ds['Time_to_relief'].iloc[0]))\nprint(type(preds))\nprint(preds.shape)\nprint(type(dtrain.target))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get rounded predictions \ndtrain[\"predround\"] = preds.round().astype(int)\ndtrain.predround[:5]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get delta between predictions on training set and actual training target values\n# hand calculate accuracy on training set as ratio of (total training samples - wrong training predictions)/total training samples\n\ndeltatr = abs(dtrain.target[:100000] - dtrain.predround[:100000])\ndeltatr[:50]\nprint(deltatr.sum())\nprint(\"percentage correct train\")\nprint((len(deltatr) - deltatr.sum())/len(deltatr))\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# predict values for validation X values\n# X_valid, dvalid.target\npredval = model.predict(X_valid, batch_size=BATCH_SIZE)\ndvalid[\"predround\"] = predval.round().astype(int)\ndvalid[\"predict\"] = predval\n#print(type(deltaval))\n#print(len(deltaval))\ndvalid.predict[:5]\n\n\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# hand calculation of proportion correct guesses in validation set\n\ndvalid[\"deltaval\"] = abs(dvalid.target - dvalid.predround)\nprint(dvalid[\"deltaval\"][:10])\nprint(dvalid[\"deltaval\"].sum())\n# print(\"percentage correct\")\n# print((len(deltaval) - deltaval.sum())/len(deltaval))\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get subset of dataframe with wrong guesses\n# k1 = df.loc[(df.Product == p_id)\ndvalidwrong = dvalid.loc[(dvalid.deltaval == 1)]\ndvalidright = dvalid.loc[(dvalid.deltaval == 0)]\ndvalidwrong.head(20)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dvalidright.head(20)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# dvalid.hist(range = (0,5))\n# apar_ds.Time_to_relief.hist(range = (0,5))\ndvalid.predict.hist()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# ,encoding = \"ISO-8859-1\"\nif dsxmode == False:\n    dvalidwrong.to_csv('/home/paperspace/data/idugexample/dvalidwrongjan27.csv')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# from io \nimport BytesIO \nimport requests \nimport json \nimport pandas as pd \n\ndef put_file(credentials, local_file_name):  \n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage V3.\"\"\"\n    f = open(local_file_name,'r')\n    my_data = f.read()\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n            'password': credentials['password']}}}}}\n    headers1 = {'Content-Type': 'application/csv'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    print(resp1)\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', credentials['container'], '/', local_file_name])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.put(url=url2, headers=headers2, data = my_data )\n    print(resp2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# DSX code to save correct and incorrect datasets\ncredentials_51 = {\n  'auth_url':'https://identity.open.softlayer.com',\n  'project':'object_storage_50d2807d_af9f_4d26_be1b_a21ead8aaf92',\n  'project_id':'1845e915f01446f08cd6e04ec7ecef1c',\n  'region':'dallas',\n  'user_id':'d54f40f14b8147cf9a53cca0125a4c60',\n  'domain_id':'79758d227a684a349da10cb1ec70f102',\n  'domain_name':'1261715',\n  'username':'member_d942dbebfe2c93f84ad42a8be103a296d45b3025',\n  'password':\"\"\"S{)JKjGK&6,.oQ4I\"\"\",\n  'container':'DutyManagerJan2018',\n  'filename':'dvalidrightjan27.csv'\n}\n# pd.read_csv(get_object_storage_file_with_credentials_50d2807daf9f4d26be1ba21ead8aaf92('DutyManagerJan2018', 'PMRs2012toJan2018j15.csv')\ndvalidright.to_csv('dvalidrightjan27.csv',index=False)\nput_file(credentials_51,'dvalidrightjan27.csv')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def get_object_storage_file_with_credentials_50d2807daf9f4d26be1ba21ead8aaf92(container, filename):\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n                                      'password': {'user': {'name': 'member_d942dbebfe2c93f84ad42a8be103a296d45b3025','domain': {'id': '79758d227a684a349da10cb1ec70f102'},\n            'password': 'S{)JKjGK&6,.oQ4I'}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', container, '/', filename])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.get(url=url2, headers=headers2)\n    return StringIO(resp2.text)\n\nif dsxmode:\n    list_ = []\n    apar_ds2 = pd.DataFrame()\n    # read in subset of columns required for Db2\n    apar_ds1 = pd.read_csv(get_object_storage_file_with_credentials_50d2807daf9f4d26be1ba21ead8aaf92('DutyManagerJan2018', 'PMRs2012toJan2018j15.csv'),usecols=['Country','Prob_Comp_Current_Release_Level','Component_ID','Initial_Severity_Code'\n                   ,'Days_Open','ENG_Indicator','Apar_IP_Orig_Indicator','Owner_ID','Back_End_Hours_Indicator','Customer'\n                   ,'Resolver_ID','Time_to_Relief','Prob_Abstract_Text','System_Down_Indicator'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dvalidwrong.to_csv('dvalidwrongjan27.csv',index=False)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dtrain.target[:50]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\ndf_data_1 = pd.read_csv(get_object_storage_file_with_credentials_50d2807daf9f4d26be1ba21ead8aaf92('DutyManagerJan2018', 'pmrs2016to2017prepdljan8.csv'))\ndf_data_1.head()\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# chart accuracy and loss for train and validation sets\n\nprint(modelfit.history.keys())\n#  acc\nplt.plot(modelfit.history['acc'])\nplt.plot(modelfit.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# Loss\nplt.plot(modelfit.history['loss'])\nplt.plot(modelfit.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%%time"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# basic test evaluation\n# score = model.evaluate(x_test,y_test, batch_size=32)\n# Returns the loss value & metrics values for the model in test mode.\ntest[\"target\"] = test[\"TTRgtthresh\"]\nscores = model.evaluate(X_test, test.target, batch_size=200)\nmodel.metrics_names\n# for x in range(0, 3):\nprint()\nfor i in range(0,len(model.metrics_names)):\n    print(\"%s: %.2f%%\" % (model.metrics_names[i], scores[i]*100))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model.metrics_names"
        }, 
        {
            "source": "# Kaggle submission that was used as input for this notebook\nhttps://www.kaggle.com/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Summary\nThis notebook shows methods for dealing with structured data in the context of a neural network.\n\n# Author\n\nMark Ryan is a manager at IBM Canada.\n\nCopyright \u00a9 IBM Corp. 2018. This notebook and its source code are released under the terms of the MIT License.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}