{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Applying Deep Learning to solve a Client Success problem - Duty Manager call prediction\n\nGOAL: tackle a day-to-day Db2 Client Success problem using deep learning\n\nPROBLEM CHOSEN: predict likelihood a ticket will generate a duty manager call\n\nDATA SOURCE: ticketing dashboard (ticket details) \n\nFEATURES: TBD\n    \n\n\nAPPROACH: start with a basic, working DL model and simplify/augment to optimizer prediction accuracy\n- using this article on structured DL (i.e. DL on structured data, as opposed to common DL examples on unstructured data like images & audio) https://towardsdatascience.com/structured-deep-learning-b8ca4138b848 as a starting point, look for some examples of end-to-end DL on structured data. This article refers to a Kaggle competition on structured data, an entry in which https://www.kaggle.com/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl , for example, is a great end-to-end example\n- use Keras (Python DL framework built on top of TensorFlow) - sweet spot for a simple, working DL model. Keras has decent documentation and a big enough community that answers to many questions are readily available\n- once input data working through model, tune the following parameters to optimize validation/test accuracy (accuracy of prediction on data not used to generate model weights):\n- learning rate (to avoid local minima and oscillation)\n- dropout rate (control overfitting by randomly dropping nodes)\n- lambda for L2 regularization (control overfitting by \"suppressing\" large weights)\n- batch size (the number of training examples in a forward/backward pass)\n- epochs run (number of complete passes through dataset)\n\nENVIRONMENTS USED:\n- public cloud DSX: for most development and testing after initial setup and prior to adding non-Db2 data. PROS: convenient, persistent sessions, no cost; CONS: performance ~ 30% slower than PaperSpace, memory errors trying to run model on larger data set including text analysis\n- Paperspace (https://www.paperspace.com/) low-cost generic GPU-enabled Linux environemnt. Used instructions from Jeremy Howard fast.ai course V2 to setup. PROS: better perf, able to handle text processing w. largest data set without memory errors, standard filesystem; CONS: setup not straightforward, inconvenience of having to start & stop environment\n- initial development done on Paperspace, moved to DSX once the basics were working, then back to Paperspace after the larger data set began to generate memory errors in DSX\n\nSOURCE DATA :\n\nOVERVIEW OF CODE:\n- ingest CSVs: need distinct code for DSX and paper\n- various cleansing / slicing and dicing to generate additional features (e.g. year and day of week from date opened). Several variations attempted (e.g. make severity a continuous variable, distort severity at extremes, exclude some severities, exclude some TTR ranges) \n- define label / target: new column in dataframe that is 0 or 1 depending on whether TTR is less than or more than 1 day\n- split data into training (which will be further split into training/validation) and test data sets (training data used to generate model weights; validation set used to track how well model predicts actual targets as hyperparameters are tuned; test set used to validate entire model (learned weights and hyperparameters)\n- fill in missing values for all features\n- in categorical columns (all feature columns currently) replace values (e.g. \"CANADA\", \"JAPAN\" for Country) with integer IDs\n- process text columns - replace words with vector of integer IDs\n- define Keras model:\n- embedding layers for all categorical features (embeddings are vectors used to learn patterns for categorical data - https://en.wikipedia.org/wiki/Word_embedding )\n- RNN layer for text feature\n- single dense output layer w. sigmoid activation to output TTR prediction\n\nCURRENT HWMs:\n\nVARIATIONS ATTEMPTED:\n\nDL Glossary: https://deeplearning4j.org/glossary", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# set overall parameters\n\ndsxmode = True # overall is this being run in DSX?\n\ntestproportion = 0.01 # proportion of data reserved for test set\ntrainproportion = 0.8 # proportion of non-test data dedicated to training (vs. validation)\nverboseout = False\nincludetext = True # switch to determine whether text fields are included in model\ntargetthresh = 6.0\nemptythresh = 1000\n# to address imbalance in training data between zero (above targetthresh) and detractor (below targetthresh) specify weight in compile and fit\n# class_weight = {0 : zero_weight, 1: one_weight}\n# consider calculating these values from actual skew rather than hard-coding them here\nzero_weight = 1.0\none_weight = 4.5\n\n# hyperparameters\nlearning_rate = 0.001\ndropout_rate = 0.00003 #0.003\nl2_lambda = 0.00003 #7.5\n"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
                }
            ], 
            "source": "# common imports\nimport zipfile\nimport pandas as pd\nimport numpy as np\nimport time\nimport datetime\nfrom dateutil import relativedelta\nfrom io import StringIO\nimport pandas as pd\n\n# DSX code to import uploaded documents\nfrom io import StringIO\nimport requests\nimport json\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.cross_validation import train_test_split\n\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport math\nfrom subprocess import check_output\nimport seaborn as sns\n"
        }, 
        {
            "source": "# Data input\n\nRead in data from external source:\n- DSX - access data from DWoC table\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 3, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CASE_NUMBER</th>\n      <th>SUBJECT</th>\n      <th>OWNER_LAST_NAME</th>\n      <th>ACCOUNT_NAME_TEXT</th>\n      <th>CONTACT_NAME</th>\n      <th>ACCOUNT_PRIORITY</th>\n      <th>LEGACY_PROBLEM_NUMBER</th>\n      <th>SEVERITY_LEVEL</th>\n      <th>CASE_OWNER_ALIAS</th>\n      <th>BLUE_DIAMOND_ACCOUNT</th>\n      <th>COUNTRY</th>\n      <th>WATSON_SKILL_SUGGESTION_(DEPRECATED)</th>\n      <th>STATUS</th>\n      <th>DATE_OPENED</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TS000003163</td>\n      <td>FMP exiting with error and HeadExtentID 260611...</td>\n      <td>Ma</td>\n      <td>BELASTINGDIENST CAO</td>\n      <td>Vannisselroy, Koen</td>\n      <td>Analytics-GEP</td>\n      <td>28,585,211,788</td>\n      <td>3 - Minor business impact</td>\n      <td>majason</td>\n      <td>No</td>\n      <td>NL</td>\n      <td>None</td>\n      <td>Closed by IBM</td>\n      <td>08/11/2017</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TS000003166</td>\n      <td>W R db2 load process hang for loading 5.8T rec...</td>\n      <td>BADIGER</td>\n      <td>CA Franchise Tax Board</td>\n      <td>Jiang, Lily</td>\n      <td>None</td>\n      <td>65,923,227,000</td>\n      <td>2 - Significant impact (any system is down)</td>\n      <td>sbad</td>\n      <td>No</td>\n      <td>US</td>\n      <td>None</td>\n      <td>Closed - Archived</td>\n      <td>09/06/2017</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TS000007186</td>\n      <td>UPDATE SYSSTAT.INDEXES hangs</td>\n      <td>Koranteng-Asante</td>\n      <td>Lloyds Banking Group Service Delivery, Group IT</td>\n      <td>Nelson, Philip</td>\n      <td>None</td>\n      <td>Not Applicable</td>\n      <td>2 - Significant impact (any system is down)</td>\n      <td>asan</td>\n      <td>No</td>\n      <td>GB</td>\n      <td>None</td>\n      <td>Closed - Archived</td>\n      <td>9/21/2017</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TS000007630</td>\n      <td>Notified Load operation fails after revoking P...</td>\n      <td>Koranteng-Asante</td>\n      <td>STATE OF CONNECTICUT</td>\n      <td>McCabe, Barbara</td>\n      <td>Analytics-Industry</td>\n      <td>Not Applicable</td>\n      <td>2 - Significant impact (any system is down)</td>\n      <td>asan</td>\n      <td>No</td>\n      <td>US</td>\n      <td>None</td>\n      <td>Waiting for IBM</td>\n      <td>9/26/2017</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TS000027965</td>\n      <td>Sporadic SQL SP slow performance due to invali...</td>\n      <td>Ma</td>\n      <td>21ST CENTURY INSURANCE AND FINANCIAL SERVICES</td>\n      <td>Pentyala, Dileep Kumar</td>\n      <td>Analytics-Industry</td>\n      <td>Not Applicable</td>\n      <td>2 - Significant impact (any system is down)</td>\n      <td>majason</td>\n      <td>No</td>\n      <td>US</td>\n      <td>None</td>\n      <td>Waiting for IBM</td>\n      <td>11/01/2017</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   CASE_NUMBER                                            SUBJECT  \\\n0  TS000003163  FMP exiting with error and HeadExtentID 260611...   \n1  TS000003166  W R db2 load process hang for loading 5.8T rec...   \n2  TS000007186                       UPDATE SYSSTAT.INDEXES hangs   \n3  TS000007630  Notified Load operation fails after revoking P...   \n4  TS000027965  Sporadic SQL SP slow performance due to invali...   \n\n    OWNER_LAST_NAME                                ACCOUNT_NAME_TEXT  \\\n0                Ma                              BELASTINGDIENST CAO   \n1           BADIGER                           CA Franchise Tax Board   \n2  Koranteng-Asante  Lloyds Banking Group Service Delivery, Group IT   \n3  Koranteng-Asante                             STATE OF CONNECTICUT   \n4                Ma    21ST CENTURY INSURANCE AND FINANCIAL SERVICES   \n\n             CONTACT_NAME    ACCOUNT_PRIORITY LEGACY_PROBLEM_NUMBER  \\\n0      Vannisselroy, Koen       Analytics-GEP        28,585,211,788   \n1             Jiang, Lily                None        65,923,227,000   \n2          Nelson, Philip                None        Not Applicable   \n3         McCabe, Barbara  Analytics-Industry        Not Applicable   \n4  Pentyala, Dileep Kumar  Analytics-Industry        Not Applicable   \n\n                                SEVERITY_LEVEL CASE_OWNER_ALIAS  \\\n0                    3 - Minor business impact          majason   \n1  2 - Significant impact (any system is down)             sbad   \n2  2 - Significant impact (any system is down)             asan   \n3  2 - Significant impact (any system is down)             asan   \n4  2 - Significant impact (any system is down)          majason   \n\n  BLUE_DIAMOND_ACCOUNT COUNTRY WATSON_SKILL_SUGGESTION_(DEPRECATED)  \\\n0                   No      NL                                 None   \n1                   No      US                                 None   \n2                   No      GB                                 None   \n3                   No      US                                 None   \n4                   No      US                                 None   \n\n              STATUS DATE_OPENED  \n0      Closed by IBM  08/11/2017  \n1  Closed - Archived  09/06/2017  \n2  Closed - Archived   9/21/2017  \n3    Waiting for IBM   9/26/2017  \n4    Waiting for IBM  11/01/2017  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 4, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CASE_NUMBER</th>\n      <th>CASE_OWNER_ALIAS</th>\n      <th>OPENED_DATE</th>\n      <th>SUPPORT_MISSION</th>\n      <th>PRODUCT_NAME</th>\n      <th>ACCOUNT_NAME</th>\n      <th>SUBJECT</th>\n      <th>CREATED_BY</th>\n      <th>LEGACY_PROBLEM_NUMBER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TS000797520</td>\n      <td>Db2 Adv Supp</td>\n      <td>4/20/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>Wyndham Worldwide</td>\n      <td>SQL Replicaton using for DB migration</td>\n      <td>Srikanth Vangaveti</td>\n      <td>Not Applicable</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TS000797566</td>\n      <td>Kevi</td>\n      <td>4/20/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>MASTERCARD TECHNOLOGIES LLC</td>\n      <td>Ingest Utility</td>\n      <td>Tom Glaser</td>\n      <td>Not Applicable</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TS000797583</td>\n      <td>Jerry</td>\n      <td>4/20/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>TaiKang Insurance Group CO. LTD</td>\n      <td>Attn Pals(NeedHelp)|GCGTSC@ZY:Db2:</td>\n      <td>Support ETL</td>\n      <td>92,185,124,672</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TS000797592</td>\n      <td>aball</td>\n      <td>4/20/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>Kansas City Southern Railway Company, The</td>\n      <td>Performance</td>\n      <td>Victor Burke</td>\n      <td>Not Applicable</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TS000797627</td>\n      <td>jhas</td>\n      <td>4/20/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>Morgan Stanley &amp; Company, Inc.</td>\n      <td>Can't force application (duplicate TS000161444)</td>\n      <td>jianyu lu</td>\n      <td>Not Applicable</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   CASE_NUMBER CASE_OWNER_ALIAS OPENED_DATE SUPPORT_MISSION  \\\n0  TS000797520     Db2 Adv Supp   4/20/2018     DB2 for LUW   \n1  TS000797566             Kevi   4/20/2018     DB2 for LUW   \n2  TS000797583            Jerry   4/20/2018     DB2 for LUW   \n3  TS000797592            aball   4/20/2018     DB2 for LUW   \n4  TS000797627             jhas   4/20/2018     DB2 for LUW   \n\n                  PRODUCT_NAME                               ACCOUNT_NAME  \\\n0  Db2 Linux, Unix and Windows                          Wyndham Worldwide   \n1  Db2 Linux, Unix and Windows                MASTERCARD TECHNOLOGIES LLC   \n2  Db2 Linux, Unix and Windows            TaiKang Insurance Group CO. LTD   \n3  Db2 Linux, Unix and Windows  Kansas City Southern Railway Company, The   \n4  Db2 Linux, Unix and Windows             Morgan Stanley & Company, Inc.   \n\n                                           SUBJECT          CREATED_BY  \\\n0            SQL Replicaton using for DB migration  Srikanth Vangaveti   \n1                                   Ingest Utility          Tom Glaser   \n2               Attn Pals(NeedHelp)|GCGTSC@ZY:Db2:         Support ETL   \n3                                      Performance        Victor Burke   \n4  Can't force application (duplicate TS000161444)           jianyu lu   \n\n  LEGACY_PROBLEM_NUMBER  \n0        Not Applicable  \n1        Not Applicable  \n2        92,185,124,672  \n3        Not Applicable  \n4        Not Applicable  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 5, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(96, 14)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dm_cases.shape"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 6, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(7086, 9)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "merged_data.shape"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 7, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CASE_NUMBER</th>\n      <th>SUBJECT</th>\n      <th>OWNER_LAST_NAME</th>\n      <th>ACCOUNT_NAME_TEXT</th>\n      <th>CONTACT_NAME</th>\n      <th>ACCOUNT_PRIORITY</th>\n      <th>LEGACY_PROBLEM_NUMBER</th>\n      <th>SEVERITY_LEVEL</th>\n      <th>CASE_OWNER_ALIAS</th>\n      <th>BLUE_DIAMOND_ACCOUNT</th>\n      <th>COUNTRY</th>\n      <th>WATSON_SKILL_SUGGESTION_(DEPRECATED)</th>\n      <th>STATUS</th>\n      <th>DATE_OPENED</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TS000003163</td>\n      <td>FMP exiting with error and HeadExtentID 260611...</td>\n      <td>Ma</td>\n      <td>BELASTINGDIENST CAO</td>\n      <td>Vannisselroy, Koen</td>\n      <td>Analytics-GEP</td>\n      <td>28,585,211,788</td>\n      <td>3 - Minor business impact</td>\n      <td>majason</td>\n      <td>No</td>\n      <td>NL</td>\n      <td>None</td>\n      <td>Closed by IBM</td>\n      <td>08/11/2017</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TS000003166</td>\n      <td>W R db2 load process hang for loading 5.8T rec...</td>\n      <td>BADIGER</td>\n      <td>CA Franchise Tax Board</td>\n      <td>Jiang, Lily</td>\n      <td>None</td>\n      <td>65,923,227,000</td>\n      <td>2 - Significant impact (any system is down)</td>\n      <td>sbad</td>\n      <td>No</td>\n      <td>US</td>\n      <td>None</td>\n      <td>Closed - Archived</td>\n      <td>09/06/2017</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TS000007186</td>\n      <td>UPDATE SYSSTAT.INDEXES hangs</td>\n      <td>Koranteng-Asante</td>\n      <td>Lloyds Banking Group Service Delivery, Group IT</td>\n      <td>Nelson, Philip</td>\n      <td>None</td>\n      <td>Not Applicable</td>\n      <td>2 - Significant impact (any system is down)</td>\n      <td>asan</td>\n      <td>No</td>\n      <td>GB</td>\n      <td>None</td>\n      <td>Closed - Archived</td>\n      <td>9/21/2017</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TS000007630</td>\n      <td>Notified Load operation fails after revoking P...</td>\n      <td>Koranteng-Asante</td>\n      <td>STATE OF CONNECTICUT</td>\n      <td>McCabe, Barbara</td>\n      <td>Analytics-Industry</td>\n      <td>Not Applicable</td>\n      <td>2 - Significant impact (any system is down)</td>\n      <td>asan</td>\n      <td>No</td>\n      <td>US</td>\n      <td>None</td>\n      <td>Waiting for IBM</td>\n      <td>9/26/2017</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TS000027965</td>\n      <td>Sporadic SQL SP slow performance due to invali...</td>\n      <td>Ma</td>\n      <td>21ST CENTURY INSURANCE AND FINANCIAL SERVICES</td>\n      <td>Pentyala, Dileep Kumar</td>\n      <td>Analytics-Industry</td>\n      <td>Not Applicable</td>\n      <td>2 - Significant impact (any system is down)</td>\n      <td>majason</td>\n      <td>No</td>\n      <td>US</td>\n      <td>None</td>\n      <td>Waiting for IBM</td>\n      <td>11/01/2017</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   CASE_NUMBER                                            SUBJECT  \\\n0  TS000003163  FMP exiting with error and HeadExtentID 260611...   \n1  TS000003166  W R db2 load process hang for loading 5.8T rec...   \n2  TS000007186                       UPDATE SYSSTAT.INDEXES hangs   \n3  TS000007630  Notified Load operation fails after revoking P...   \n4  TS000027965  Sporadic SQL SP slow performance due to invali...   \n\n    OWNER_LAST_NAME                                ACCOUNT_NAME_TEXT  \\\n0                Ma                              BELASTINGDIENST CAO   \n1           BADIGER                           CA Franchise Tax Board   \n2  Koranteng-Asante  Lloyds Banking Group Service Delivery, Group IT   \n3  Koranteng-Asante                             STATE OF CONNECTICUT   \n4                Ma    21ST CENTURY INSURANCE AND FINANCIAL SERVICES   \n\n             CONTACT_NAME    ACCOUNT_PRIORITY LEGACY_PROBLEM_NUMBER  \\\n0      Vannisselroy, Koen       Analytics-GEP        28,585,211,788   \n1             Jiang, Lily                None        65,923,227,000   \n2          Nelson, Philip                None        Not Applicable   \n3         McCabe, Barbara  Analytics-Industry        Not Applicable   \n4  Pentyala, Dileep Kumar  Analytics-Industry        Not Applicable   \n\n                                SEVERITY_LEVEL CASE_OWNER_ALIAS  \\\n0                    3 - Minor business impact          majason   \n1  2 - Significant impact (any system is down)             sbad   \n2  2 - Significant impact (any system is down)             asan   \n3  2 - Significant impact (any system is down)             asan   \n4  2 - Significant impact (any system is down)          majason   \n\n  BLUE_DIAMOND_ACCOUNT COUNTRY WATSON_SKILL_SUGGESTION_(DEPRECATED)  \\\n0                   No      NL                                 None   \n1                   No      US                                 None   \n2                   No      GB                                 None   \n3                   No      US                                 None   \n4                   No      US                                 None   \n\n              STATUS DATE_OPENED  target  \n0      Closed by IBM  08/11/2017       1  \n1  Closed - Archived  09/06/2017       1  \n2  Closed - Archived   9/21/2017       1  \n3    Waiting for IBM   9/26/2017       1  \n4    Waiting for IBM  11/01/2017       1  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# columns = ['Col1', 'Col2', ...]\n# df.drop(columns, inplace=True, axis=1)\ndm_cases[\"target\"] = 1\n\ndm_cases.head()"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 8, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "['DATE_OPENED',\n 'CONTACT_NAME',\n 'ACCOUNT_PRIORITY',\n 'LEGACY_PROBLEM_NUMBER',\n 'STATUS',\n 'SUBJECT',\n 'COUNTRY',\n 'ACCOUNT_NAME_TEXT',\n 'OWNER_LAST_NAME',\n 'SEVERITY_LEVEL',\n 'CASE_OWNER_ALIAS',\n 'WATSON_SKILL_SUGGESTION_(DEPRECATED)',\n 'BLUE_DIAMOND_ACCOUNT']"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dmcollist = list(dm_cases)\ndmexlist = ['CASE_NUMBER','target']\n# collist = list(set(nontextcols) - set(excludefromcolist) - set(nearempty))\ndmcollist = list(set(dmcollist) - set(dmexlist))\ndmcollist"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# df.drop(columns, inplace=True, axis=1)\ndm_cases.drop(dmcollist, inplace=True, axis=1)"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 10, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "1    96\nName: target, dtype: int64"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dm_cases.target.value_counts()"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 11, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(96, 2)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "dm_cases.shape"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# pd.merge(restaurant_ids_dataframe, restaurant_review_frame, \n# on='business_id', how='outer')\nmerged_data = pd.merge(merged_data,dm_cases,on= 'CASE_NUMBER', how='outer')"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 13, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "1.0    96\nName: target, dtype: int64"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "merged_data.target.value_counts()"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 14, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.0    7010\n1.0      96\nName: target, dtype: int64"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# df['column']=df['column'].fillna(value)\nmerged_data['target']=merged_data['target'].fillna(0.0)\nmerged_data.target.value_counts()"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 15, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(7106, 10)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "merged_data.shape"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 16, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CASE_NUMBER</th>\n      <th>CASE_OWNER_ALIAS</th>\n      <th>OPENED_DATE</th>\n      <th>SUPPORT_MISSION</th>\n      <th>PRODUCT_NAME</th>\n      <th>ACCOUNT_NAME</th>\n      <th>SUBJECT</th>\n      <th>CREATED_BY</th>\n      <th>LEGACY_PROBLEM_NUMBER</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TS000797520</td>\n      <td>Db2 Adv Supp</td>\n      <td>4/20/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>Wyndham Worldwide</td>\n      <td>SQL Replicaton using for DB migration</td>\n      <td>Srikanth Vangaveti</td>\n      <td>Not Applicable</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TS000797566</td>\n      <td>Kevi</td>\n      <td>4/20/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>MASTERCARD TECHNOLOGIES LLC</td>\n      <td>Ingest Utility</td>\n      <td>Tom Glaser</td>\n      <td>Not Applicable</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TS000797583</td>\n      <td>Jerry</td>\n      <td>4/20/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>TaiKang Insurance Group CO. LTD</td>\n      <td>Attn Pals(NeedHelp)|GCGTSC@ZY:Db2:</td>\n      <td>Support ETL</td>\n      <td>92,185,124,672</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TS000797592</td>\n      <td>aball</td>\n      <td>4/20/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>Kansas City Southern Railway Company, The</td>\n      <td>Performance</td>\n      <td>Victor Burke</td>\n      <td>Not Applicable</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TS000797627</td>\n      <td>jhas</td>\n      <td>4/20/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>Morgan Stanley &amp; Company, Inc.</td>\n      <td>Can't force application (duplicate TS000161444)</td>\n      <td>jianyu lu</td>\n      <td>Not Applicable</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   CASE_NUMBER CASE_OWNER_ALIAS OPENED_DATE SUPPORT_MISSION  \\\n0  TS000797520     Db2 Adv Supp   4/20/2018     DB2 for LUW   \n1  TS000797566             Kevi   4/20/2018     DB2 for LUW   \n2  TS000797583            Jerry   4/20/2018     DB2 for LUW   \n3  TS000797592            aball   4/20/2018     DB2 for LUW   \n4  TS000797627             jhas   4/20/2018     DB2 for LUW   \n\n                  PRODUCT_NAME                               ACCOUNT_NAME  \\\n0  Db2 Linux, Unix and Windows                          Wyndham Worldwide   \n1  Db2 Linux, Unix and Windows                MASTERCARD TECHNOLOGIES LLC   \n2  Db2 Linux, Unix and Windows            TaiKang Insurance Group CO. LTD   \n3  Db2 Linux, Unix and Windows  Kansas City Southern Railway Company, The   \n4  Db2 Linux, Unix and Windows             Morgan Stanley & Company, Inc.   \n\n                                           SUBJECT          CREATED_BY  \\\n0            SQL Replicaton using for DB migration  Srikanth Vangaveti   \n1                                   Ingest Utility          Tom Glaser   \n2               Attn Pals(NeedHelp)|GCGTSC@ZY:Db2:         Support ETL   \n3                                      Performance        Victor Burke   \n4  Can't force application (duplicate TS000161444)           jianyu lu   \n\n  LEGACY_PROBLEM_NUMBER  target  \n0        Not Applicable     0.0  \n1        Not Applicable     0.0  \n2        92,185,124,672     0.0  \n3        Not Applicable     0.0  \n4        Not Applicable     1.0  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "merged_data.head()"
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "md shape (7106, 10)\n"
                }
            ], 
            "source": "print(\"md shape\",merged_data.shape)"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 18, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x2ae80f2886d8>"
                    }, 
                    "output_type": "execute_result"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "image/png": "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\n", 
                        "text/plain": "<matplotlib.figure.Figure at 0x2ae80efbdd68>"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "sns.heatmap(merged_data.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[]\n"
                }
            ], 
            "source": "nearempty = []\nfor c in merged_data.columns:\n    if np.sum(merged_data[c].isnull()) >= emptythresh :\n        print(c)\n        nearempty.append(c)\nprint(nearempty)"
        }, 
        {
            "source": "# Clean up data\n\nGo through steps to prepare data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Define test / training sets; fill in missing values; encode categorical values; process text field", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Through train test split. Test proportion:\n0.01\nzero target values 7010\none target values 96\n"
                }
            ], 
            "source": "# get training and test data set\n# test_size was 0.3 up to Jan 22\n\ntrain, test = train_test_split(merged_data, test_size = testproportion)\nprint(\"Through train test split. Test proportion:\")\nprint(testproportion)\nprint(\"zero target values\",(merged_data[\"target\"]==0).sum())\nprint(\"one target values\",(merged_data[\"target\"]==1).sum())"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "all cols ['CASE_NUMBER', 'CASE_OWNER_ALIAS', 'OPENED_DATE', 'SUPPORT_MISSION', 'PRODUCT_NAME', 'ACCOUNT_NAME', 'SUBJECT', 'CREATED_BY', 'LEGACY_PROBLEM_NUMBER', 'target']\n"
                }
            ], 
            "source": "allcols = list(merged_data)\nprint(\"all cols\",allcols)"
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "allcols 10\nnearempty 0\ntextcols 1\ncollist 7\n"
                }
            ], 
            "source": "# define the required column lists\n#textcols = ['Other_Reason_for_Cancellation', 'Reason_for_Reopening', 'Resolution_Description', 'Subject']\ntextcols = ['SUBJECT']\nexcludefromcolist = ['CASE_NUMBER','target']\n# list(set(temp1) - set(temp2))\nnontextcols = list(set(allcols) - set(textcols))\ncollist = list(set(nontextcols) - set(excludefromcolist) - set(nearempty))\n\n# print column list lengths:\nprint(\"allcols\",len(allcols))\nprint(\"nearempty\",len(nearempty))\nprint(\"textcols\",len(textcols))\nprint(\"collist\",len(collist))"
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "testo\nlength collist 7\ncollist ['PRODUCT_NAME', 'ACCOUNT_NAME', 'LEGACY_PROBLEM_NUMBER', 'SUPPORT_MISSION', 'CASE_OWNER_ALIAS', 'OPENED_DATE', 'CREATED_BY']\n"
                }
            ], 
            "source": "\n\nprint(\"testo\")\nprint(\"length collist\",len(collist))\nprint(\"collist\",collist)\n"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "before mv\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/pandas/core/generic.py:4355: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._update_inplace(new_data)\n"
                }, 
                {
                    "execution_count": 24, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CASE_NUMBER</th>\n      <th>CASE_OWNER_ALIAS</th>\n      <th>OPENED_DATE</th>\n      <th>SUPPORT_MISSION</th>\n      <th>PRODUCT_NAME</th>\n      <th>ACCOUNT_NAME</th>\n      <th>SUBJECT</th>\n      <th>CREATED_BY</th>\n      <th>LEGACY_PROBLEM_NUMBER</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1378</th>\n      <td>TS000145499</td>\n      <td>STBG</td>\n      <td>3/15/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>BIOTRONIK SE &amp; Co. KG</td>\n      <td>eDM export fail with errors</td>\n      <td>Smit Kalwal</td>\n      <td>Not Applicable</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2579</th>\n      <td>TS000174500</td>\n      <td>NAKA</td>\n      <td>04/09/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>NTT DATA SEKISUI SYSTEMS CORPORATION</td>\n      <td>fix pack????????</td>\n      <td>Hiroaki Nakatsubo</td>\n      <td>Not Applicable</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4370</th>\n      <td>TS000164886</td>\n      <td>juil</td>\n      <td>04/02/2018</td>\n      <td>DB2 for LUW</td>\n      <td>Db2 Linux, Unix and Windows</td>\n      <td>KTB Computer Services Co., LTD</td>\n      <td>db2ckupgrade Fail</td>\n      <td>sanprasirt boonma</td>\n      <td>Not Applicable</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "      CASE_NUMBER CASE_OWNER_ALIAS OPENED_DATE SUPPORT_MISSION  \\\n1378  TS000145499             STBG   3/15/2018     DB2 for LUW   \n2579  TS000174500             NAKA  04/09/2018     DB2 for LUW   \n4370  TS000164886             juil  04/02/2018     DB2 for LUW   \n\n                     PRODUCT_NAME                          ACCOUNT_NAME  \\\n1378  Db2 Linux, Unix and Windows                 BIOTRONIK SE & Co. KG   \n2579  Db2 Linux, Unix and Windows  NTT DATA SEKISUI SYSTEMS CORPORATION   \n4370  Db2 Linux, Unix and Windows        KTB Computer Services Co., LTD   \n\n                          SUBJECT         CREATED_BY LEGACY_PROBLEM_NUMBER  \\\n1378  eDM export fail with errors        Smit Kalwal        Not Applicable   \n2579             fix pack????????  Hiroaki Nakatsubo        Not Applicable   \n4370            db2ckupgrade Fail  sanprasirt boonma        Not Applicable   \n\n      target  \n1378     0.0  \n2579     0.0  \n4370     0.0  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# fill missing values\nprint(\"before mv\")\ndef fill_missing(dataset,collist):\n    for col in collist:\n        dataset[col].fillna(value=\"missing\", inplace=True)\n    return (dataset)\n\ntrain = fill_missing(train,allcols)\ntest = fill_missing(test,allcols)\ntrain.head(3)\n"
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
                }, 
                {
                    "execution_count": 25, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CASE_NUMBER</th>\n      <th>CASE_OWNER_ALIAS</th>\n      <th>OPENED_DATE</th>\n      <th>SUPPORT_MISSION</th>\n      <th>PRODUCT_NAME</th>\n      <th>ACCOUNT_NAME</th>\n      <th>SUBJECT</th>\n      <th>CREATED_BY</th>\n      <th>LEGACY_PROBLEM_NUMBER</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1378</th>\n      <td>TS000145499</td>\n      <td>86</td>\n      <td>67</td>\n      <td>0</td>\n      <td>4</td>\n      <td>254</td>\n      <td>eDM export fail with errors</td>\n      <td>1530</td>\n      <td>2435</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2579</th>\n      <td>TS000174500</td>\n      <td>70</td>\n      <td>33</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1340</td>\n      <td>fix pack????????</td>\n      <td>581</td>\n      <td>2435</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4370</th>\n      <td>TS000164886</td>\n      <td>154</td>\n      <td>26</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1113</td>\n      <td>db2ckupgrade Fail</td>\n      <td>1960</td>\n      <td>2435</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6782</th>\n      <td>TS000837941</td>\n      <td>72</td>\n      <td>97</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1613</td>\n      <td>DB2 Start Errors</td>\n      <td>415</td>\n      <td>2435</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6895</th>\n      <td>TS000179568</td>\n      <td>50</td>\n      <td>36</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1277</td>\n      <td>Web????????????????????</td>\n      <td>1603</td>\n      <td>1112</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "      CASE_NUMBER  CASE_OWNER_ALIAS  OPENED_DATE  SUPPORT_MISSION  \\\n1378  TS000145499                86           67                0   \n2579  TS000174500                70           33                0   \n4370  TS000164886               154           26                0   \n6782  TS000837941                72           97                0   \n6895  TS000179568                50           36                0   \n\n      PRODUCT_NAME  ACCOUNT_NAME                      SUBJECT  CREATED_BY  \\\n1378             4           254  eDM export fail with errors        1530   \n2579             4          1340             fix pack????????         581   \n4370             4          1113            db2ckupgrade Fail        1960   \n6782             4          1613             DB2 Start Errors         415   \n6895             4          1277      Web????????????????????        1603   \n\n      LEGACY_PROBLEM_NUMBER  target  \n1378                   2435     0.0  \n2579                   2435     0.0  \n4370                   2435     0.0  \n6782                   2435     0.0  \n6895                   1112     0.0  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# process categorical data\n# replace values with category IDs in the following columns\n# \n\n\nle = LabelEncoder()\n\nfor col in collist:\n    if verboseout:\n        print(\"processing \",col)\n    le.fit(np.hstack([train[col], test[col]]))\n    train[col] = le.transform(train[col])\n    test[col] = le.transform(test[col])\n    \ndel le\n\n\ntrain.head(5)\n    "
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 26, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "1378    eDM export fail with errors\n2579               fix pack????????\n4370              db2ckupgrade Fail\nName: SUBJECT, dtype: object"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "train['SUBJECT'].head(3)"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
                }
            ], 
            "source": "# part 1 of text processing\n# tokenize list of text columns (made up of multiple strings)\n\nfrom keras.preprocessing.text import Tokenizer\n\n# text columns that we care about\n# textcols = ['Other_Reason_for_Cancellation', 'Reason_for_Reopening', 'Resolution_Description', 'Subject']\n# textcols = ['Resolution_Description', 'Subject']\n\nfor col in textcols:\n    if verboseout:\n        print(\"processing text col\",col)\n    raw_text = train[col].str.lower()\n    tok_raw = Tokenizer()\n    tok_raw.fit_on_texts(raw_text)\n    train[col] = tok_raw.texts_to_sequences(train[col].str.lower())\n    test[col] = tok_raw.texts_to_sequences(test[col].str.lower())\n\n"
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 28, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "1378                            [2823, 302, 303, 12, 108]\n2579                                           [164, 195]\n4370                                           [551, 303]\n6782                                        [1, 125, 108]\n6895                                                [718]\n6010    [3, 222, 1, 880, 619, 9, 4, 75, 278, 1, 304, 7...\n1971             [179, 19, 26, 1933, 27, 39, 7, 2, 33, 6]\n5314    [47, 20, 104, 2, 1537, 285, 20, 460, 796, 19, ...\n3676                                      [174, 45, 2824]\n4856                 [26, 17, 130, 37, 259, 663, 13, 101]\nName: SUBJECT, dtype: object"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "train['SUBJECT'].head(10)"
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 29, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'\\nprint(\"res decr max\",train[\\'Resolution_Description\\'].max())\\nprint(\"subject\",train[\\'SUBJECT\\'].max())\\nprint(\"orc\",train[\\'Other_Reason_for_Cancellation\\'].max())\\nprint(\"Reason_for_Reopening\",train[\\'Reason_for_Reopening\\'].max())\\nprint(\"cmr number max\",train[\\'CMR_Number\\'].max())\\nprint(\"npmaxer \",np.max([np.max(train[\\'Resolution_Description\\'].max()), np.max(train[\\'Subject\\'].max()),np.max(train[\\'Other_Reason_for_Cancellation\\'].max()),np.max(train[\\'Reason_for_Reopening\\'].max())])) \\nprint(\"test npmaxer \",np.max([np.max(test[\\'Resolution_Description\\'].max()), np.max(test[\\'Subject\\'].max()),np.max(test[\\'Other_Reason_for_Cancellation\\'].max()),np.max(test[\\'Reason_for_Reopening\\'].max())])) \\n#print(\"overall npmaxer \",np.max([np.max(merged_data[\\'Resolution_Description\\'].max()), np.max(merged_data[\\'Subject\\'].max()),np.max(merged_data[\\'Other_Reason_for_Cancellation\\'].max()),np.max(merged_data[\\'Reason_for_Reopening\\'].max())]))\\n'"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# textcols = ['Other_Reason_for_Cancellation', 'Reason_for_Reopening', 'Resolution_Description', 'Subject']\n'''\nprint(\"res decr max\",train['Resolution_Description'].max())\nprint(\"subject\",train['SUBJECT'].max())\nprint(\"orc\",train['Other_Reason_for_Cancellation'].max())\nprint(\"Reason_for_Reopening\",train['Reason_for_Reopening'].max())\nprint(\"cmr number max\",train['CMR_Number'].max())\nprint(\"npmaxer \",np.max([np.max(train['Resolution_Description'].max()), np.max(train['Subject'].max()),np.max(train['Other_Reason_for_Cancellation'].max()),np.max(train['Reason_for_Reopening'].max())])) \nprint(\"test npmaxer \",np.max([np.max(test['Resolution_Description'].max()), np.max(test['Subject'].max()),np.max(test['Other_Reason_for_Cancellation'].max()),np.max(test['Reason_for_Reopening'].max())])) \n#print(\"overall npmaxer \",np.max([np.max(merged_data['Resolution_Description'].max()), np.max(merged_data['Subject'].max()),np.max(merged_data['Other_Reason_for_Cancellation'].max()),np.max(merged_data['Reason_for_Reopening'].max())]))\n'''"
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# part 2 of text processing\n# max_abstract_seq = np.max([np.max(train.seq_abstract.apply(lambda x: len(x))), np.max(test.seq_abstract.apply(lambda x: len(x)))])\n\n# print(\"max name seq \"+str(max_abstract_seq))\n"
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "textmax 6620\n"
                }
            ], 
            "source": "# max values for embeddings\n\n\n\nmax_dict = {}\ntextmax = 50\n\nfor col in collist:\n    max_dict[col] = np.max([train[col].max(), test[col].max()])+1\n    \n# np.max([np.max(train['Resolution_Description'].max()), np.max(train['Subject'].max()),np.max(train['Other_Reason_for_Cancellation'].max()),np.max(train['Reason_for_Reopening'].max())])) \nfor cols in textcols:\n    max_dict[cols] = np.max([np.max(train[cols].max()), np.max(test[cols].max())])+10\n    if max_dict[cols] > textmax:\n        textmax = max_dict[cols]\n\nprint(\"textmax\",textmax)\n                             \nif verboseout:\n    print(\"max_dict\",max_dict)\n\n"
        }, 
        {
            "source": "# Split training set into train / validate", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "(5627, 10)\n(1407, 10)\n2222    0.0\n3191    0.0\n27      0.0\n6677    0.0\nName: target, dtype: float64\n4305    0.0\n6344    0.0\n6644    0.0\n3086    0.0\nName: target, dtype: float64\n"
                }
            ], 
            "source": "# define and scale target and get validation sets\n# apar_ds['TTRgtthresh'] = np.where(apar_ds['Time_to_relief'] >= ttrthresh,1,0)\n\n# train[\"target\"] = np.where(traintrain[\"LIKELIHOOD_TO_RECOMMEND\"]\n\n# train[\"target\"] = train[\"Time_to_relief\"]\n\n#train[\"target\"] = np.log(train.Time_to_relief+1)\n#target_scaler = MinMaxScaler(feature_range=(-1, 1))\n#train[\"target\"] = target_scaler.fit_transform(train.target.reshape(-1,1))\n#pd.DataFrame(train.target).hist()\n\n# train_size 0.8 up to Jan 22\ndtrain, dvalid = train_test_split(train, random_state=123, train_size=trainproportion)\n\n# modelfit = model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n#          , validation_data=(X_valid, dvalid.target), verbose=1)\n\nprint(dtrain.shape)\nprint(dvalid.shape)\nif verboseout:\n    train[\"target\"].head(10)\nelse:\n    #trn_labels[:4]\n    print(dtrain[\"target\"][:4])\n    print(dvalid.target[:4])\n"
        }, 
        {
            "execution_count": 33, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "keras variables defined\n"
                }
            ], 
            "source": "# define keras variables\nfrom keras.preprocessing.sequence import pad_sequences\n\n# 'Country','Release','Comp_name','Dmcall','Severity','English_Spoken','Owner_ID','BE_Indicator','Customer','Technical_Resolver_ID'\n# MAX_COUNTRY ,MAX_RELEASE ,MAX_COMP_NAME ,MAX_DMCALL ,MAX_SEVERITY ,MAX_ENGLISH_SPOKEN ,MAX_OWNER_ID ,MAX_BE_INDICATOR ,MAX_CUSTOMER ,MAX_TECHNICAL_RESOLVER_ID \n# FOLLOW UP - put this in a loop so it's not hard coded\n\n# X for the features used\n\ndef get_keras_vars(dataset):\n    X = {}\n    for col in collist:\n        if verboseout:\n            print(\"cat col is\",col)\n        X[col] = np.array(dataset[col])\n   \n    for col in textcols:\n        if verboseout:\n            print(\"text col is\",col)\n        X[col] = pad_sequences(dataset[col], maxlen=max_dict[col])\n        \n    return X\n\nX_train = get_keras_vars(dtrain)\nX_valid = get_keras_vars(dvalid)\nX_test = get_keras_vars(test)\nprint(\"keras variables defined\")\n\n\n\n"
        }, 
        {
            "execution_count": 34, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# shortcut for quick hyperparameter tweaks\n# l2_lambda = 7.5\n# dropout_rate = 0.03\n# learning_rate = 0.001\n# textcols = ['Other_Reason_for_Cancellation', 'Reason_for_Reopening', 'Resolution_Description', 'Subject']\n\n\ndropout_rate = 0.0003\nl2_lambda = 0.0003"
        }, 
        {
            "source": "# Define & run model\n\nIdentify model input, layers, and compilation details. \n\nRun model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 35, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "about to define embeddings\nabout to define embeddings\ntextmax is 6620\ncol SUBJECT\ntext input shape 6620\nmax in the midst 6620\nthrough loops for cols\nthrough definition of non-text parts of main_l\nmain_l Tensor(\"concatenate_7/concat:0\", shape=(?, ?), dtype=float32)\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nACCOUNT_NAME (InputLayer)       (None, 1)            0                                            \n__________________________________________________________________________________________________\nCASE_OWNER_ALIAS (InputLayer)   (None, 1)            0                                            \n__________________________________________________________________________________________________\nembedding_2 (Embedding)         (None, 1, 10)        19930       ACCOUNT_NAME[0][0]               \n__________________________________________________________________________________________________\nembedding_5 (Embedding)         (None, 1, 10)        2430        CASE_OWNER_ALIAS[0][0]           \n__________________________________________________________________________________________________\nPRODUCT_NAME (InputLayer)       (None, 1)            0                                            \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 1, 10)        40          embedding_2[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 1, 10)        40          embedding_5[0][0]                \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 1, 10)        220         PRODUCT_NAME[0][0]               \n__________________________________________________________________________________________________\nLEGACY_PROBLEM_NUMBER (InputLay (None, 1)            0                                            \n__________________________________________________________________________________________________\nflatten_1 (Flatten)             (None, 10)           0           batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nflatten_2 (Flatten)             (None, 10)           0           batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 1, 10)        40          embedding_1[0][0]                \n__________________________________________________________________________________________________\nembedding_3 (Embedding)         (None, 1, 10)        24370       LEGACY_PROBLEM_NUMBER[0][0]      \n__________________________________________________________________________________________________\nSUPPORT_MISSION (InputLayer)    (None, 1)            0                                            \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 10)           0           flatten_1[0][0]                  \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 10)           0           flatten_2[0][0]                  \n__________________________________________________________________________________________________\nflatten_3 (Flatten)             (None, 10)           0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 1, 10)        40          embedding_3[0][0]                \n__________________________________________________________________________________________________\nembedding_4 (Embedding)         (None, 1, 10)        20          SUPPORT_MISSION[0][0]            \n__________________________________________________________________________________________________\nOPENED_DATE (InputLayer)        (None, 1)            0                                            \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 20)           0           dropout_2[0][0]                  \n                                                                 dropout_3[0][0]                  \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 10)           0           flatten_3[0][0]                  \n__________________________________________________________________________________________________\nflatten_4 (Flatten)             (None, 10)           0           batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 1, 10)        40          embedding_4[0][0]                \n__________________________________________________________________________________________________\nembedding_6 (Embedding)         (None, 1, 10)        1050        OPENED_DATE[0][0]                \n__________________________________________________________________________________________________\nCREATED_BY (InputLayer)         (None, 1)            0                                            \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 30)           0           concatenate_1[0][0]              \n                                                                 dropout_4[0][0]                  \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 10)           0           flatten_4[0][0]                  \n__________________________________________________________________________________________________\nflatten_5 (Flatten)             (None, 10)           0           batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 1, 10)        40          embedding_6[0][0]                \n__________________________________________________________________________________________________\nembedding_7 (Embedding)         (None, 1, 10)        20030       CREATED_BY[0][0]                 \n__________________________________________________________________________________________________\nSUBJECT (InputLayer)            (None, 6620)         0                                            \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 40)           0           concatenate_2[0][0]              \n                                                                 dropout_5[0][0]                  \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 10)           0           flatten_5[0][0]                  \n__________________________________________________________________________________________________\nflatten_6 (Flatten)             (None, 10)           0           batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 1, 10)        40          embedding_7[0][0]                \n__________________________________________________________________________________________________\nembedding_8 (Embedding)         (None, 6620, 50)     331000      SUBJECT[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 50)           0           concatenate_3[0][0]              \n                                                                 dropout_6[0][0]                  \n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 10)           0           flatten_6[0][0]                  \n__________________________________________________________________________________________________\nflatten_7 (Flatten)             (None, 10)           0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 6620, 50)     200         embedding_8[0][0]                \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 60)           0           concatenate_4[0][0]              \n                                                                 dropout_7[0][0]                  \n__________________________________________________________________________________________________\ndropout_8 (Dropout)             (None, 10)           0           flatten_7[0][0]                  \n__________________________________________________________________________________________________\ngru_1 (GRU)                     (None, 16)           3216        batch_normalization_8[0][0]      \n__________________________________________________________________________________________________\nconcatenate_6 (Concatenate)     (None, 70)           0           concatenate_5[0][0]              \n                                                                 dropout_8[0][0]                  \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 16)           0           gru_1[0][0]                      \n__________________________________________________________________________________________________\nconcatenate_7 (Concatenate)     (None, 86)           0           concatenate_6[0][0]              \n                                                                 dropout_1[0][0]                  \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1)            87          concatenate_7[0][0]              \n==================================================================================================\nTotal params: 402,833\nTrainable params: 402,593\nNon-trainable params: 240\n__________________________________________________________________________________________________\n"
                }
            ], 
            "source": "# define model in keras\n\n# basic imports\nfrom keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import regularizers\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\nfrom keras import backend as K\nfrom keras.utils.vis_utils import plot_model\n\ndef onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())\n\n# original lambda l2_lambda = 0.0001\n# for 5 x increase in dataset, increase lambda 5x\n\n\ndef get_callbacks(filepath, patience=2):\n    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [es, msave]\n\n\ndef get_model():\n    \n   \n    #Inputs\n    # name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    #Prob_Abstract_Text = Input(shape=[X_train[\"Prob_Abstract_Text\"].shape[1]], name=\"Prob_Abstract_Text\")\n    # Country = Input(shape=[1], name=\"Country\")\n    # Release = Input(shape=[1], name=\"Release\")\n    #Release_Linear = Input(shape=[1], name=\"Release_Linear\")\n    #Report_Date_days = Input(shape=[1], name=\"Report_Date_days\")\n    \n    catinputs = {}\n    textinputs = {}\n    embeddings = {}\n    textembeddings = {}\n    catemb = 10\n    textemb = 50\n    \n    print(\"about to define embeddings\")\n    \n    print(\"about to define embeddings\")\n    collistfix = []\n    textlayerlist = []\n    inputlayerlist = []\n    i = 0\n    print(\"textmax is\",textmax)\n    for col in collist:\n        catinputs[col] = Input(shape=[1],name=col)\n        inputlayerlist.append(catinputs[col])\n        #print(\"inputname\",inputname)\n        #print(\"type inputname\",type(inputname))\n        #print(\"catinputs[col] type\", type(catinputs[col]))\n        #catinputs[col] = Input(shape=[1],name=col)\n        embeddings[col] = (Embedding(max_dict[col],catemb) (catinputs[col]))\n        # batchnorm all \n        embeddings[col] = (BatchNormalization() (embeddings[col]))\n        collistfix.append(embeddings[col])\n        \n    if includetext:    \n        for col in textcols:\n            print(\"col\",col)\n            textinputs[col] = Input(shape=[X_train[col].shape[1]], name=col)\n            print(\"text input shape\",X_train[col].shape[1])\n            inputlayerlist.append(textinputs[col])\n            textembeddings[col] = (Embedding(textmax,textemb) (textinputs[col]))\n            textembeddings[col] = (BatchNormalization() (textembeddings[col])) \n            textembeddings[col] = Dropout(dropout_rate) ( GRU(16,kernel_regularizer=l2(l2_lambda)) (textembeddings[col]))\n            collistfix.append(textembeddings[col])\n            print(\"max in the midst\",np.max([np.max(train[col].max()), np.max(test[col].max())])+10)\n        print(\"through loops for cols\")\n        #textcols = ['Other_Reason_for_Cancellation', 'Reason_for_Reopening', 'Resolution_Description', 'Subject']\n        \n    \n    \n    #rnn layer here if there were text fields\n        \n    # rnn_layer1 = Dropout(dropout_rate) ( GRU(16,kernel_regularizer=l2(l2_lambda)) (emb_Abstract))\n    # rnn_layer2 = GRU(8) (emb_name)\n    \n    #concatlist = []\n    #for cols in collist:\n    #    concatlist = concatlist+(Dropout(dropout_rate) (Flatten() (embeddings[cols])))\n    #print(\"concatlist\",concatlist)\n    # 'ACCOUNT_NAME', 'CASE_OWNER_ALIAS'\n          \n    # main_l = concatenate([\n    main_l = concatenate([Dropout(dropout_rate) (Flatten() (embeddings['ACCOUNT_NAME']) ),Dropout(dropout_rate) (Flatten() (embeddings['CASE_OWNER_ALIAS']) )])\n    for cols in collist:\n        if (cols != 'ACCOUNT_NAME') & (cols != 'CASE_OWNER_ALIAS'):\n            main_l = concatenate([main_l,Dropout(dropout_rate) (Flatten() (embeddings[cols]) )])\n    \n    print(\"through definition of non-text parts of main_l\")\n    if includetext:\n        for col in textcols:\n            main_l = concatenate([main_l,textembeddings[col]])\n                                                 \n                                                 \n    print(\"main_l\", main_l)                                            \n    \n    # originally 2 dense layers here, sizes were 128 / 64 \n    #main_l2 = Dropout(dr_r) (Dense(128,kernel_regularizer=l2(l2_lambda)) (main_l))\n    # main_l3 = Dropout(dropout_rate) (Dense(32,kernel_regularizer=l2(l2_lambda)) (main_l))\n    \n    #output\n    # change to softmax for TTRthreshold\n    #output = Dense(1, activation=\"linear\") (main_l)\n    #output = Dense(1, activation=\"softmax\") (main_l)\n    \n    \n    #output = Dense(1, activation=\"sigmoid\") (main_l)\n    output = Dense(1, activation=\"sigmoid\") (main_l)\n    # output = Dense(1, activation=\"linear\") (main_l)\n    \n    #model\n        \n    # model = Model([Country,Release,Comp_name,Initial_Severity_Linear, Owner_ID,Customer,Technical_Resolver_ID, System_Down_Indicator, Prob_Abstract_Text], output)\n    # model = Model([Country,Release,Product,Initial_Severity_Linear, Owner_ID,Customer,Technical_Resolver_ID, Prob_Abstract_Text], output)\n    #model = Model([Country,Release_Linear,Product,Initial_Severity_Linear, Owner_ID,Customer,Technical_Resolver_ID, Prob_Abstract_Text], output)\n    # [Country, Report_Date_days,Comp_name,Initial_Severity_Linear, Owner_ID,Customer,Technical_Resolver_ID, Prob_Abstract_Text, Release_Linear]\n                                                   \n    model = Model(inputlayerlist, output)\n    \n    # model = Model([Country,Release,Product,Initial_Severity_Linear,Customer,Technical_Resolver_ID, Prob_Abstract_Text], output)\n    # optimizer = SGD(lr=0.01, momentum=0.4)\n    \n    \n    # optimizer = Adam(lr=learning_rate)\n    optimizer = SGD(lr=learning_rate)\n   \n    # optimizer = SGD(lr=0.5)\n    # model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"], weighted_metrics=[\"accuracy\"])\n    # model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n    \n      \n    \n    return model\n\n    \nmodel = get_model()\nif dsxmode == False:\n    plot_model(model, to_file='/home/paperspace/visualizations/model_plotfeb15.png', show_shapes=True, show_layer_names=True)\nmodel.summary()\n    \n\n    "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "text cols ['SUBJECT']\ndropout  0.0003\nL2 lambda  0.0003\nbatch size  10\nabout to define embeddings\nabout to define embeddings\ntextmax is 6620\ncol SUBJECT\ntext input shape 6620\nmax in the midst 6620\nthrough loops for cols\nthrough definition of non-text parts of main_l\nmain_l Tensor(\"concatenate_14/concat:0\", shape=(?, ?), dtype=float32)\nTrain on 5627 samples, validate on 1407 samples\nEpoch 1/10\n  60/5627 [..............................] - ETA: 4:58:58 - loss: 0.6539 - acc: 0.6333 - weighted_acc: 0.6333"
                }
            ], 
            "source": "BATCH_SIZE = 10\nepochs = 10\nprint(\"text cols\",textcols)\nprint(\"dropout \",dropout_rate)\nprint(\"L2 lambda \",l2_lambda)\nprint(\"batch size \",BATCH_SIZE)\n\n\nmodel = get_model()\nmodelfit = model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n         , validation_data=(X_valid, dvalid.target),class_weight = {0 : zero_weight, 1: one_weight}, verbose=1)\n# modelfit = model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n#           , validation_data=(X_valid, dvalid.target), verbose=1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "if dsxmode == False:\n    model.save('/home/paperspace/models/TTRmodelfeb15b.h5')\n# model = load_model('my_model.h5')"
        }, 
        {
            "source": "# Predictions and renderings", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# functions to parse and manipulate dates in the style of the input CSVs\nimport datetime\nfrom datetime import date\ndef create_date(year,month):\n    outdate = datetime.date(year,month,15)\n    return(outdate)\n\ndef parse_bic_date(bic_date_in):\n    year = int(bic_date_in[0:4])\n    month = int(bic_date_in[-2:])\n    return(year,month)\n\ndef create_date_from_bic(bic_date_in):\n    yr,mth = parse_bic_date(bic_date_in)\n    retdate = create_date(yr,mth)\n    return retdate\n\ndef get_datecomp_from_csvdate (csv_date):\n    day = int(csv_date[0:2])\n    # month_number = datetime.datetime.strptime(month_name, '%b').month\n    month = datetime.datetime.strptime(csv_date[3:6], '%b').month\n    year = int('20'+csv_date[-2:])\n    # year = int(csv_date[-2:])\n    return (year,month,day)\n\ndef get_date_from_csvdate (csv_date):\n    day = int(csv_date[0:2])\n    # month_number = datetime.datetime.strptime(month_name, '%b').month\n    month = datetime.datetime.strptime(csv_date[3:6], '%b',coerce=True).month\n    year = int('20'+csv_date[-2:])\n    # year = int(csv_date[-2:])\n    return (date(year,month,day))\n\ndef get_year_from_csvdate (csv_date):\n    year = int('20'+csv_date[-2:])\n    return (year)\n\ndef get_month_from_csvdate (csv_date):\n    month = datetime.datetime.strptime(csv_date[3:6], '%b',coerce=True).month\n    return (month)\n\ndef get_day_from_csvdate (csv_date):\n    day = int(csv_date[0:2])\n    return (day)\n\ndef get_weekday (date):\n    return(date.weekday())\n\n# pd.to_datetime(x, coerce=True)\n\ndef validatedate(csv_text):\n    try:\n        datetime.datetime.strptime(csv_date[3:6], '%b')\n    except ValueError:\n        raise ValueError(\"Incorrect data format, should be YYYY-MM-DD\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# predictions on training set\n\npreds = model.predict(X_train, batch_size=BATCH_SIZE)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "preds[:50]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dtrain[\"predict\"] = preds\ndtrain.predict[:5]\nif verboseout:\n    dtrain.predict.hist()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "if verboseout:\n    dtrain.predict.hist(bins=2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "if verboseout:\n    dvalid.predict.hist(bins=2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# print(type(apar_ds['Time_to_relief'].iloc[0]))\nprint(type(preds))\nprint(preds.shape)\nprint(type(dtrain.target))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get rounded predictions \ndtrain[\"predround\"] = preds.round().astype(int)\ndtrain.predround[:5]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get delta between predictions on training set and actual training target values\n# hand calculate accuracy on training set as ratio of (total training samples - wrong training predictions)/total training samples\n\ndeltatr = abs(dtrain.target[:100000] - dtrain.predround[:100000])\ndeltatr[:50]\nprint(deltatr.sum())\nprint(\"percentage correct train\")\nprint((len(deltatr) - deltatr.sum())/len(deltatr))\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# predict values for validation X values\n# X_valid, dvalid.target\npredval = model.predict(X_valid, batch_size=BATCH_SIZE)\ndvalid[\"predround\"] = predval.round().astype(int)\ndvalid[\"predict\"] = predval\n#print(type(deltaval))\n#print(len(deltaval))\ndvalid.predict[:5]\n\n\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# hand calculation of proportion correct guesses in validation set\n\ndvalid[\"deltaval\"] = abs(dvalid.target - dvalid.predround)\nprint(dvalid[\"deltaval\"][:10])\nprint(dvalid[\"deltaval\"].sum())\n# print(\"percentage correct\")\n# print((len(deltaval) - deltaval.sum())/len(deltaval))\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# get subset of dataframe with wrong guesses\n# k1 = df.loc[(df.Product == p_id)\ndvalidwrong = dvalid.loc[(dvalid.deltaval == 1)]\ndvalidright = dvalid.loc[(dvalid.deltaval == 0)]\ndvalidwrong.head(20)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dvalidright.head(20)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# dvalid.hist(range = (0,5))\n# apar_ds.Time_to_relief.hist(range = (0,5))\ndvalid.predict.hist()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# ,encoding = \"ISO-8859-1\"\nif dsxmode == False:\n    dvalidwrong.to_csv('/home/paperspace/data/idugexample/dvalidwrongjan27.csv')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# from io \nimport BytesIO \nimport requests \nimport json \nimport pandas as pd \n\ndef put_file(credentials, local_file_name):  \n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage V3.\"\"\"\n    f = open(local_file_name,'r')\n    my_data = f.read()\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': credentials['username'],'domain': {'id': credentials['domain_id']},\n            'password': credentials['password']}}}}}\n    headers1 = {'Content-Type': 'application/csv'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    print(resp1)\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', credentials['container'], '/', local_file_name])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.put(url=url2, headers=headers2, data = my_data )\n    print(resp2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# DSX code to save correct and incorrect datasets\ncredentials_51 = {\n  'auth_url':'https://identity.open.softlayer.com',\n  'project':'object_storage_50d2807d_af9f_4d26_be1b_a21ead8aaf92',\n  'project_id':'1845e915f01446f08cd6e04ec7ecef1c',\n  'region':'dallas',\n  'user_id':'d54f40f14b8147cf9a53cca0125a4c60',\n  'domain_id':'79758d227a684a349da10cb1ec70f102',\n  'domain_name':'1261715',\n  'username':'member_d942dbebfe2c93f84ad42a8be103a296d45b3025',\n  'password':\"\"\"S{)JKjGK&6,.oQ4I\"\"\",\n  'container':'DutyManagerJan2018',\n  'filename':'dvalidrightjan27.csv'\n}\n# pd.read_csv(get_object_storage_file_with_credentials_50d2807daf9f4d26be1ba21ead8aaf92('DutyManagerJan2018', 'PMRs2012toJan2018j15.csv')\ndvalidright.to_csv('dvalidrightjan27.csv',index=False)\nput_file(credentials_51,'dvalidrightjan27.csv')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def get_object_storage_file_with_credentials_50d2807daf9f4d26be1ba21ead8aaf92(container, filename):\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n                                      'password': {'user': {'name': 'member_d942dbebfe2c93f84ad42a8be103a296d45b3025','domain': {'id': '79758d227a684a349da10cb1ec70f102'},\n            'password': 'S{)JKjGK&6,.oQ4I'}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', container, '/', filename])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.get(url=url2, headers=headers2)\n    return StringIO(resp2.text)\n\nif dsxmode:\n    list_ = []\n    apar_ds2 = pd.DataFrame()\n    # read in subset of columns required for Db2\n    apar_ds1 = pd.read_csv(get_object_storage_file_with_credentials_50d2807daf9f4d26be1ba21ead8aaf92('DutyManagerJan2018', 'PMRs2012toJan2018j15.csv'),usecols=['Country','Prob_Comp_Current_Release_Level','Component_ID','Initial_Severity_Code'\n                   ,'Days_Open','ENG_Indicator','Apar_IP_Orig_Indicator','Owner_ID','Back_End_Hours_Indicator','Customer'\n                   ,'Resolver_ID','Time_to_Relief','Prob_Abstract_Text','System_Down_Indicator'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dvalidwrong.to_csv('dvalidwrongjan27.csv',index=False)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dtrain.target[:50]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\ndf_data_1 = pd.read_csv(get_object_storage_file_with_credentials_50d2807daf9f4d26be1ba21ead8aaf92('DutyManagerJan2018', 'pmrs2016to2017prepdljan8.csv'))\ndf_data_1.head()\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# chart accuracy and loss for train and validation sets\n\nprint(modelfit.history.keys())\n#  acc\nplt.plot(modelfit.history['acc'])\nplt.plot(modelfit.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# Loss\nplt.plot(modelfit.history['loss'])\nplt.plot(modelfit.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "%%time"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# basic test evaluation\n# score = model.evaluate(x_test,y_test, batch_size=32)\n# Returns the loss value & metrics values for the model in test mode.\ntest[\"target\"] = test[\"TTRgtthresh\"]\nscores = model.evaluate(X_test, test.target, batch_size=200)\nmodel.metrics_names\n# for x in range(0, 3):\nprint()\nfor i in range(0,len(model.metrics_names)):\n    print(\"%s: %.2f%%\" % (model.metrics_names[i], scores[i]*100))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "model.metrics_names"
        }, 
        {
            "source": "# Kaggle submission that was used as input for this notebook\nhttps://www.kaggle.com/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Summary\nThis notebook shows methods for dealing with structured data in the context of a neural network.\n\n# Author\n\nMark Ryan is a manager at IBM Canada.\n\nCopyright \u00a9 IBM Corp. 2018. This notebook and its source code are released under the terms of the MIT License.", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}